{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "api = wandb.Api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = api.projects(entity=\"haraghi\")\n",
    "for project in projects:\n",
    "    print(project.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name_and_num_classes = {\n",
    "    \"NCARS\": {\"name\": \"N-Cars\", \"num_classes\": 2},\n",
    "    \"NASL\": {\"name\": \"N-ASL\", \"num_classes\": 24},\n",
    "    \"NCALTECH101\": {\"name\": \"N-Caltech101\", \"num_classes\": 101},\n",
    "    \"DVSGESTURE_TONIC\": {\"name\": \"DVS-Gesture\", \"num_classes\": 11},\n",
    "    \"FAN1VS3\": {\"name\": \"Fan1vs3\", \"num_classes\": 2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_projects = [\n",
    "        \"FINAL-NASL-varyinig-sparsity\",\n",
    "        \"FINAL-NCARS-varyinig-sparsity\",\n",
    "        \"FINAL-DVSGESTURE_TONIC-HP-varyinig-sparsity\",\n",
    "        \"FINAL-FAN1VS3-varyinig-sparsity\",\n",
    "        \"FINAL-NCALTECH101-varyinig-sparsity\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_val_and_test_acc_keys(run):\n",
    "    val_acc_key = []\n",
    "    test_acc_key = []\n",
    "    for key in run.summary.keys():\n",
    "        if \"val\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            val_acc_key.append(key)\n",
    "        if \"test\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            test_acc_key.append(key)\n",
    "    assert len(val_acc_key) <= 1, f\"More than one val acc key found: {val_acc_key}\"\n",
    "    assert len(test_acc_key) <= 1, f\"More than one test acc key found: {test_acc_key}\"\n",
    "    return val_acc_key[0] if len(val_acc_key) == 1 else None , test_acc_key[0] if len(test_acc_key) == 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'paper'\n",
    "subfolder_name = os.path.join('images',folder_name,'sparsity_vs_acc')\n",
    "entity = 'haraghi'\n",
    "if not os.path.exists(subfolder_name):\n",
    "    os.makedirs(subfolder_name)\n",
    "file_path = os.path.join(subfolder_name,\"sparsity_vs_acc.tex\")\n",
    "file_path_md = os.path.join(subfolder_name,\"sparsity_vs_acc.md\")\n",
    "\n",
    "\n",
    "val_dict = {}\n",
    "test_dict = {}\n",
    "num_events_set = set() \n",
    "\n",
    "for project_name in dataset_projects:\n",
    "    runs = api.runs(f\"{entity}/{project_name}\")\n",
    "    runs = [r for r in runs if r.state == \"finished\" and \"transform\" in r.config]\n",
    "    if len(runs) == 0:\n",
    "        print(f\"No runs found for {project_name}\")\n",
    "        continue\n",
    "    num_events = np.unique([run.config['transform']['train']['num_events_per_sample'] for run in runs])\n",
    "    runs_per_num_events = {num_event: [run for run in runs if run.config['transform']['train']['num_events_per_sample'] == num_event] for num_event in num_events}\n",
    "    dataset_name = runs[0].config[\"dataset\"][\"name\"]\n",
    "    \n",
    "    num_events_set = num_events_set.union(set(num_events))\n",
    "    \n",
    "    val_mean = {}\n",
    "    test_mean = {}\n",
    "    lr = {}\n",
    "    batch_size = {}\n",
    "    weight_decay = {}\n",
    "    \n",
    "    for num_event in num_events:\n",
    "        val_mean[num_event] = []\n",
    "        test_mean[num_event] = []\n",
    "        lr[num_event] = []\n",
    "        batch_size[num_event] = []\n",
    "        weight_decay[num_event] = []\n",
    "        for run in runs_per_num_events[num_event]:\n",
    "            val_key, test_key = find_val_and_test_acc_keys(run)\n",
    "            val_mean[num_event].append(run.summary[val_key] if val_key in run.summary else None)\n",
    "            test_mean[num_event].append(run.summary[test_key] if test_key in run.summary else None)\n",
    "            lr[num_event].append(run.config['optimize']['lr'])\n",
    "            batch_size[num_event].append(run.config['train']['batch_size'])\n",
    "            if 'weight_decay' in run.config['optimize']:\n",
    "                weight_decay[num_event].append(run.config['optimize']['weight_decay'])\n",
    "                \n",
    "        print(f\"percentage of runs with val acc for {num_event} events: {np.sum([v is not None for v in val_mean[num_event]]) / len(val_mean[num_event])} out of {len(val_mean[num_event])} runs\")\n",
    "        print(f\"percentage of runs with test acc for {num_event} events: {np.sum([v is not None for v in test_mean[num_event]]) / len(test_mean[num_event])} out of {len(test_mean[num_event])} runs\")\n",
    "    \n",
    "    val_dict[dataset_name] = val_mean\n",
    "    test_dict[dataset_name] = test_mean  \n",
    "\n",
    "num_events_list = sorted(list(num_events_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_metric_mean_std(metric_dict, num_events_list):\n",
    "    full_metric_mean_std = {}\n",
    "    for dataset_name, num_events_dict in metric_dict.items():\n",
    "        full_metric_mean_std[dataset_name] = []\n",
    "        for num_events in num_events_list:\n",
    "            if num_events in num_events_dict and not any([v is None for v in num_events_dict[num_events]]):\n",
    "                full_metric_mean_std[dataset_name].append((np.mean(num_events_dict[num_events]), np.std(num_events_dict[num_events])))\n",
    "            else:\n",
    "                full_metric_mean_std[dataset_name].append((None, None))\n",
    "    return full_metric_mean_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_mean_std = create_full_metric_mean_std(test_dict, num_events_list)\n",
    "full_val_mean_std = create_full_metric_mean_std(val_dict, num_events_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sparsity_vs_acc_table(file_path, full_metric_mean_std, num_events_list, datasets_name_and_num_classes):\n",
    "    # Open file for writing\n",
    "    with open(file_path, \"w\") as file:\n",
    "        # Write table header\n",
    "        file.write(\"\\\\begin{tabular}{\"+(\"c\"*(3+len(num_events_list)))+\"}\\n\")\n",
    "        file.write(\"\\\\toprule\\n\")\n",
    "        file.write(\" & & & \\\\multicolumn{\"+str(len(num_events_list))+\"}{c}{\\\\# events per video}\\\\\\\\\\n\")\n",
    "        file.write(\"Dataset & \\\\# classes & & \" +\n",
    "                   \" & \".join([str(num_events) for num_events in num_events_list]) +\n",
    "                   \"\\\\\\\\\\n\")\n",
    "        file.write(\"\\\\midrule\\n\")\n",
    "\n",
    "        # Write table rows\n",
    "        for dataset, values in full_metric_mean_std.items():\n",
    "            row = datasets_name_and_num_classes[dataset][\"name\"] + \" & \" \n",
    "            # Number of classes\n",
    "            row += str(datasets_name_and_num_classes[dataset][\"num_classes\"]) + \" & \"\n",
    "            # Test accuracies\n",
    "            row += \"Test Acc. (\\\\%) & \"\n",
    "            \n",
    "            \n",
    "            # for mean_std_tuple in values:\n",
    "            #     if mean_std_tuple[0] is not None:\n",
    "            #         row += \"${:.2f}$ \\\\textcolor{{WildStrawberry}}{{\\\\scriptsize $\\\\pm {:.2f}$}}\".format(mean_std_tuple[0] * 100, mean_std_tuple[1] * 100) + \" & \"\n",
    "            #     else:\n",
    "            #         row += \"-- & \"\n",
    "            # file.write(row[:-2] + \"\\\\\\\\\\n\")\n",
    "            \n",
    "            for mean_std_tuple in values:\n",
    "                if mean_std_tuple[0] is not None:\n",
    "                    row += \"${:.2f}$\".format(mean_std_tuple[0] * 100) + \" & \"\n",
    "                else:\n",
    "                    row += \"-- & \"\n",
    "            file.write(row[:-2] + \"\\\\\\\\\\n\")\n",
    "            \n",
    "            row =  \" & & \"\n",
    "            row += \"\\\\textcolor{WildStrawberry}{\\\\scriptsize Std. Dev. (\\\\%)} & \"\n",
    "            for mean_std_tuple in values:\n",
    "                if mean_std_tuple[1] is not None:\n",
    "                    row += \"\\\\textcolor{{WildStrawberry}}{{\\\\scriptsize$\\\\pm {:.2f}$}}\".format(mean_std_tuple[1] * 100) + \" & \"\n",
    "                else:\n",
    "                    row += \"-- & \"                    \n",
    "            # Write the row\n",
    "            file.write(row[:-2] + \"\\\\\\\\\\n\")\n",
    "            \n",
    "            row =  \" & & \"\n",
    "            row += \"\\\\textcolor{Cerulean}{\\\\scriptsize p-value} & \"\n",
    "            for mean_std_tuple in values:\n",
    "                if mean_std_tuple[2] is not None:\n",
    "                    row += r\"\\textcolor{Cerulean}{\\scriptsize \" +  mean_std_tuple[2] + \"} & \"\n",
    "                else:\n",
    "                    row += \"-- & \"                    \n",
    "            # Write the row\n",
    "            file.write(row[:-2] + \"\\\\\\\\\\n\")\n",
    "            \n",
    "        # Write table footer\n",
    "        file.write(\"\\\\bottomrule\\n\")\n",
    "        file.write(\"\\\\end{tabular}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{{\\\\scriptsize($\\\\pm {:.2f}$)\".format(7.654))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sparsity_vs_acc_table_md(file_path, full_metric_mean_std, num_events_list, datasets_name_and_num_classes):\n",
    "    # Open file for writing\n",
    "    with open(file_path, \"w\") as file:\n",
    "        # Write table header\n",
    "  \n",
    "\n",
    "\n",
    "        file.write(\"| Dataset | # classes | \" +\n",
    "                   \" | \".join([str(num_events) for num_events in num_events_list]) +\n",
    "                   \"\\n\")\n",
    "        file.write(\"| --- \"*(2+len(num_events_list))+\"|\\n\")\n",
    "        # Write table rows\n",
    "        for dataset, values in full_metric_mean_std.items():\n",
    "            row = \"| \" + datasets_name_and_num_classes[dataset][\"name\"] + \" | \" \n",
    "            # Number of classes\n",
    "            row += str(datasets_name_and_num_classes[dataset][\"num_classes\"]) + \" | \"\n",
    "            # Test accuracies\n",
    "            for mean_std_tuple in values:\n",
    "                if mean_std_tuple[0] is not None:\n",
    "                    row += \"${:.2f}$\".format(mean_std_tuple[0] * 100) + \" ($\\\\pm {:.2f}$)\".format(mean_std_tuple[1] * 100) + \" | \"\n",
    "                else:\n",
    "                    row += \"-- | \"\n",
    "            # Write the row\n",
    "            file.write(row[:-2] + \"|\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(subfolder_name,\"full_val_mean_std.pickle\"), \"wb\") as f:\n",
    "    pickle.dump([full_val_mean_std,num_events_list], f)\n",
    "with open(os.path.join(subfolder_name,\"full_test_mean_std.pickle\"), \"wb\") as f:\n",
    "    pickle.dump([full_test_mean_std,num_events_list], f)\n",
    "with open(os.path.join(subfolder_name,\"p_values_text.pkl\"), \"rb\") as f:\n",
    "    p_values_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_mean_std_p_value = {}\n",
    "for dataset_name in full_test_mean_std.keys():\n",
    "    full_test_mean_std_p_value[dataset_name] = []\n",
    "    for i, (mean, std) in enumerate(full_test_mean_std[dataset_name]):       \n",
    "            full_test_mean_std_p_value[dataset_name].append((mean, std, p_values_text[dataset_name][i]))\n",
    "print(full_test_mean_std_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sparsity_vs_acc_table(file_path, full_test_mean_std_p_value, num_events_list, datasets_name_and_num_classes)\n",
    "write_sparsity_vs_acc_table_md(file_path_md, full_test_mean_std, num_events_list, datasets_name_and_num_classes)\n",
    "\n",
    "# Display the content of the Markdown file as a Markdown cell\n",
    "with open(file_path_md, \"r\") as file:\n",
    "    markdown_content = file.read()\n",
    "\n",
    "Markdown(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_vs_sparsity(data):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for dataset, sparsity_data in data.items():\n",
    "        sparsities = []\n",
    "        mean_accuracies = []\n",
    "        std_accuracies = []\n",
    "        \n",
    "        for sparsity, accuracies in sparsity_data.items():\n",
    "            sparsities.append(sparsity)\n",
    "            mean_accuracies.append(np.mean(accuracies))\n",
    "            std_accuracies.append(np.std(accuracies))\n",
    "        \n",
    "        plt.errorbar(sparsities, mean_accuracies, yerr=std_accuracies, label=dataset, capsize=5, marker='o', linestyle='--')\n",
    "\n",
    "    plt.xlabel('Sparsity')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Sparsity')\n",
    "    plt.xscale('log')  # Log scale if sparsity values span several orders of magnitude\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_acc_vs_sparsity(test_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
