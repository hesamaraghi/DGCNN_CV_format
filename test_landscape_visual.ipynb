{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"haraghi\"\n",
    "project = \"sweep EST (FAN1VS3) (multi val test num 20)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ids = ['bnq6i70m']\n",
    "\n",
    "run_id = run_ids[0]\n",
    "artifact_dirs = []\n",
    "for run_id in run_ids:\n",
    "    glob_results = glob(osp.join(run_id, \"checkpoints\",\"*\"))\n",
    "    assert len(glob_results) == 1\n",
    "    artifact_dirs.append(glob_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg_folder/EST_FAN1VS3_1024_wandb_sweep.yaml\n"
     ]
    }
   ],
   "source": [
    "cfg_bare = OmegaConf.load(\"config_bare.yaml\")\n",
    "configs = [api.run(osp.join(entity, project, run_id)).config for run_id in run_ids]\n",
    "cfgs = [OmegaConf.create(config) for config in configs]\n",
    "cfg_files = []\n",
    "for cfg in cfgs:\n",
    "    if \"cfg_path\" in cfg.keys():\n",
    "        print(cfg.cfg_path)\n",
    "        cfg_files.append(OmegaConf.merge(cfg_bare,OmegaConf.load(cfg.cfg_path)))\n",
    "    else:\n",
    "        cfg_files.append(cfg)\n",
    "            \n",
    "\n",
    "cfgs = [OmegaConf.merge(cfg_file, cfg) for cfg_file, cfg in zip(cfg_files, cfgs)]\n",
    "# print(OmegaConf.to_yaml(cfgs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_dict_compare(all_cfg, other_cfg):\n",
    "    \"\"\"\n",
    "    Recursively compare two dictionaries and return their differences.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    diff = {}\n",
    "\n",
    "    # Check for keys in dict1 that are not in dict2\n",
    "    for key in other_cfg:\n",
    "        if key not in all_cfg:\n",
    "            diff[key] = other_cfg[key]\n",
    "        else:\n",
    "            # If the values are dictionaries, recursively compare them\n",
    "            if isinstance(all_cfg[key], dict) and isinstance(other_cfg[key], dict):\n",
    "                nested_diff = recursive_dict_compare(all_cfg[key], other_cfg[key])\n",
    "                if nested_diff:\n",
    "                    diff[key] = nested_diff\n",
    "            # Otherwise, compare the values directly\n",
    "            elif all_cfg[key] != other_cfg[key]:\n",
    "                if not(key == \"num_classes\" and other_cfg[key] is None and all_cfg[key] is not None):\n",
    "                    diff[key] = other_cfg[key]\n",
    "                    \n",
    "\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'seed': 0, 'optimize': {'patience': 10}, 'train': {'batch_size': 8}, 'wandb': {'entity': '', 'project': ''}}]\n"
     ]
    }
   ],
   "source": [
    "print([recursive_dict_compare(OmegaConf.to_object(cfg),OmegaConf.to_object(cfg_file)) for cfg, cfg_file in zip(cfgs, cfg_files)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "# Seed everything. Note that this does not make training entirely\n",
    "# deterministic.\n",
    "for cfg in cfgs:\n",
    "    pl.seed_everything(cfg.seed, workers=True)\n",
    "\n",
    "for cfg in cfgs[1:]:\n",
    "    compare_dict = recursive_dict_compare(OmegaConf.to_object(cfgs[0].dataset),OmegaConf.to_object(cfg.dataset))\n",
    "    if len(compare_dict)!=0:\n",
    "        if not (len(compare_dict) == 1 and 'num_workers' in compare_dict.keys()):\n",
    "            print(compare_dict)\n",
    "            print(cfg.dataset)\n",
    "            print(cfgs[0].dataset)\n",
    "            # raise Exception(\"Datasets are not the same\")\n",
    "# Create datasets using factory pattern\n",
    "\n",
    "\n",
    "gdm = GraphDataModule(cfgs[0])\n",
    "for cfg in cfgs:\n",
    "    cfg.dataset.num_classes = gdm.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = gdm.train_dataloader()\n",
    "torch.save(trainloader, \"trainloader.pt\")\n",
    "testloader = gdm.test_dataloader()[0]\n",
    "torch.save(testloader, \"testloader.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_factory.factory(cfg) for cfg in cfgs]\n",
    "\n",
    "# Tie it all together with PyTorch Lightning: Runner contains the model,\n",
    "# optimizer, loss function and metrics; Trainer executes the\n",
    "# training/validation loops and model checkpointing.\n",
    " \n",
    "runners = [Runner.load_from_checkpoint(artifact_dir, cfg=cfg, model=model) for artifact_dir, cfg, model in zip(artifact_dirs, cfgs, models)]\n",
    "\n",
    "gdms = [GraphDataModule(cfg) for cfg in cfgs]\n",
    "dss = []\n",
    "for cfg,gdm in zip(cfgs, gdms):\n",
    "    cfg.dataset.num_classes = gdm.num_classes\n",
    "\n",
    "    dss.append(create_dataset(\n",
    "        dataset_path = gdm.dataset_path,\n",
    "        dataset_name  = gdm.dataset_name,\n",
    "        dataset_type = 'test',\n",
    "        transform = gdm.transform_dict['test'],\n",
    "        num_workers=gdm.num_workers\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(runners[0].model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    enable_progress_bar=True,\n",
    "    # Use DDP training by default, even for CPU training\n",
    "    # strategy=\"ddp_notebook\",\n",
    "    devices=torch.cuda.device_count(),\n",
    "    accelerator=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between runner.parameters() vs runner.stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf = torch.load(artifact_dirs[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict_set = set()\n",
    "for k,v in wtf['state_dict'].items():\n",
    "    stat_dict_set.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_set = set()\n",
    "for nmae in runners[0].named_parameters():\n",
    "    param_set.add(nmae[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.classifier.layer4.2.bn1.running_var\n",
      "model.classifier.layer1.2.bn2.num_batches_tracked\n",
      "model.classifier.layer2.1.bn1.running_mean\n",
      "model.classifier.layer2.0.bn2.num_batches_tracked\n",
      "model.classifier.layer4.0.downsample.1.running_var\n",
      "model.classifier.layer3.0.bn1.num_batches_tracked\n",
      "model.classifier.layer2.0.downsample.1.running_var\n",
      "model.classifier.layer3.4.bn1.num_batches_tracked\n",
      "model.classifier.layer3.3.bn2.running_mean\n",
      "model.classifier.layer2.3.bn2.running_mean\n",
      "model.classifier.layer4.1.bn1.running_var\n",
      "model.classifier.layer2.0.downsample.1.running_mean\n",
      "model.classifier.layer4.0.bn1.running_var\n",
      "model.classifier.layer2.0.downsample.1.num_batches_tracked\n",
      "model.classifier.layer3.0.downsample.1.running_var\n",
      "model.classifier.layer3.0.bn2.running_mean\n",
      "model.classifier.layer4.0.bn2.running_var\n",
      "model.classifier.layer3.2.bn2.running_mean\n",
      "model.classifier.layer2.2.bn1.num_batches_tracked\n",
      "model.classifier.layer3.2.bn1.num_batches_tracked\n",
      "model.classifier.layer4.2.bn2.running_mean\n",
      "model.classifier.layer2.2.bn1.running_mean\n",
      "model.classifier.layer4.1.bn2.num_batches_tracked\n",
      "model.classifier.layer3.1.bn2.running_var\n",
      "model.classifier.layer3.1.bn2.num_batches_tracked\n",
      "model.classifier.layer3.1.bn1.running_var\n",
      "model.classifier.layer3.2.bn2.num_batches_tracked\n",
      "model.classifier.layer2.1.bn1.num_batches_tracked\n",
      "model.classifier.layer3.3.bn2.running_var\n",
      "model.classifier.layer1.0.bn2.running_var\n",
      "model.classifier.layer1.0.bn1.num_batches_tracked\n",
      "model.classifier.layer3.4.bn2.running_mean\n",
      "model.classifier.layer3.0.bn2.num_batches_tracked\n",
      "model.classifier.layer2.0.bn1.running_var\n",
      "model.classifier.layer1.2.bn2.running_var\n",
      "model.classifier.layer2.1.bn2.running_var\n",
      "model.classifier.layer3.4.bn1.running_mean\n",
      "model.classifier.layer1.2.bn1.num_batches_tracked\n",
      "model.classifier.bn1.running_var\n",
      "model.classifier.layer3.5.bn1.running_mean\n",
      "model.classifier.layer4.1.bn1.running_mean\n",
      "model.classifier.layer1.2.bn1.running_var\n",
      "model.classifier.layer3.2.bn2.running_var\n",
      "model.classifier.layer1.1.bn1.running_mean\n",
      "model.classifier.layer4.0.bn1.num_batches_tracked\n",
      "model.classifier.layer3.3.bn1.running_mean\n",
      "model.classifier.layer2.2.bn2.num_batches_tracked\n",
      "model.classifier.layer4.2.bn2.running_var\n",
      "model.classifier.layer3.1.bn1.running_mean\n",
      "model.classifier.layer4.2.bn1.num_batches_tracked\n",
      "model.classifier.layer3.1.bn1.num_batches_tracked\n",
      "model.classifier.layer1.1.bn2.running_mean\n",
      "model.classifier.layer3.5.bn2.running_mean\n",
      "model.classifier.layer2.3.bn1.running_mean\n",
      "model.classifier.layer3.0.bn1.running_var\n",
      "model.classifier.layer4.0.bn1.running_mean\n",
      "model.classifier.bn1.running_mean\n",
      "model.classifier.layer2.0.bn2.running_var\n",
      "model.classifier.layer2.1.bn2.running_mean\n",
      "model.classifier.layer2.3.bn2.running_var\n",
      "model.classifier.layer1.1.bn1.running_var\n",
      "model.classifier.layer4.0.downsample.1.num_batches_tracked\n",
      "model.classifier.layer4.1.bn1.num_batches_tracked\n",
      "model.classifier.layer1.0.bn1.running_mean\n",
      "model.classifier.layer4.2.bn1.running_mean\n",
      "model.classifier.layer3.5.bn1.num_batches_tracked\n",
      "model.classifier.layer3.1.bn2.running_mean\n",
      "model.classifier.layer2.0.bn1.num_batches_tracked\n",
      "model.classifier.bn1.num_batches_tracked\n",
      "model.classifier.layer1.1.bn1.num_batches_tracked\n",
      "model.classifier.layer2.3.bn1.running_var\n",
      "model.classifier.layer3.5.bn1.running_var\n",
      "model.classifier.layer4.0.bn2.running_mean\n",
      "model.classifier.layer3.4.bn2.num_batches_tracked\n",
      "model.classifier.layer4.1.bn2.running_mean\n",
      "model.classifier.layer3.4.bn2.running_var\n",
      "model.classifier.layer1.0.bn2.running_mean\n",
      "model.classifier.layer4.2.bn2.num_batches_tracked\n",
      "model.classifier.layer1.1.bn2.running_var\n",
      "model.classifier.layer2.3.bn2.num_batches_tracked\n",
      "model.classifier.layer3.3.bn1.num_batches_tracked\n",
      "model.classifier.layer4.0.bn2.num_batches_tracked\n",
      "model.classifier.layer2.1.bn2.num_batches_tracked\n",
      "model.classifier.layer3.0.downsample.1.running_mean\n",
      "model.classifier.layer3.2.bn1.running_mean\n",
      "model.classifier.layer1.2.bn2.running_mean\n",
      "model.classifier.layer2.2.bn1.running_var\n",
      "model.classifier.layer2.1.bn1.running_var\n",
      "model.classifier.layer2.0.bn2.running_mean\n",
      "model.classifier.layer3.2.bn1.running_var\n",
      "model.classifier.layer2.2.bn2.running_mean\n",
      "model.classifier.layer2.2.bn2.running_var\n",
      "model.classifier.layer3.5.bn2.running_var\n",
      "model.classifier.layer4.0.downsample.1.running_mean\n",
      "model.classifier.layer3.3.bn2.num_batches_tracked\n",
      "model.classifier.layer1.1.bn2.num_batches_tracked\n",
      "model.classifier.layer3.0.downsample.1.num_batches_tracked\n",
      "model.classifier.layer3.3.bn1.running_var\n",
      "model.classifier.layer1.0.bn2.num_batches_tracked\n",
      "model.classifier.layer3.0.bn1.running_mean\n",
      "model.classifier.layer2.3.bn1.num_batches_tracked\n",
      "model.classifier.layer4.1.bn2.running_var\n",
      "model.classifier.layer2.0.bn1.running_mean\n",
      "model.classifier.layer1.2.bn1.running_mean\n",
      "model.classifier.layer3.0.bn2.running_var\n",
      "model.classifier.layer3.5.bn2.num_batches_tracked\n",
      "model.classifier.layer3.4.bn1.running_var\n",
      "model.classifier.layer1.0.bn1.running_var\n"
     ]
    }
   ],
   "source": [
    "difference = stat_dict_set - param_set\n",
    "for d in difference:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'EST', 'k': None, 'aggr': '', 'num_bins': 9, 'cnn_type': 'resnet34', 'resnet_crop_dimension': [224, 224], 'est_mlp_layers': [1, 30, 30, 1], 'est_activation': 'nn.LeakyReLU(negative_slope=0.1)', 'resnet_pretrained': True}\n",
      "Testing DataLoader 19: 100%|██████████| 3/3 [00:00<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 19: 100%|██████████| 3/3 [00:00<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9866d43880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hesam/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hesam/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/hesam/miniconda3/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/hesam/miniconda3/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/hesam/miniconda3/lib/python3.10/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/hesam/miniconda3/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 8: 100%|██████████| 3/3 [00:00<00:00,  8.35it/s]"
     ]
    }
   ],
   "source": [
    "num_test = 10\n",
    "\n",
    "\n",
    "for runner in runners:\n",
    "    print(runner.cfg.model)\n",
    "    acc_list = []\n",
    "    duration_list = []\n",
    "    for i in range(num_test):\n",
    "        start_time = time.time()\n",
    "        test_results = trainer.test(runner, datamodule=gdm, verbose=False)\n",
    "        end_time = time.time()\n",
    "        duration_list.append(end_time-start_time)\n",
    "        acc_list.append(test_results[0]['test/acc_mean'])\n",
    "    print(f\"acc: {np.mean(acc_list)} +- {np.std(acc_list)}\")\n",
    "    print(f\"duration: {np.mean(duration_list)} +- {np.std(duration_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAN1VS3(78)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdm.test_dataloader()[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have a list of accuracy values called `acc_list`\n",
    "plt.hist(acc_list)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Accuracy')\n",
    "plt.show()\n",
    "print(np.mean(acc_list))\n",
    "print(np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "            create_dataset(\n",
    "                dataset_path = gdm.dataset_path,\n",
    "                dataset_name  = gdm.dataset_name,\n",
    "                dataset_type = 'test',\n",
    "                transform = gdm.transform_dict['test'],\n",
    "                num_workers=gdm.num_workers\n",
    "            ),\n",
    "            batch_size=cfg.train.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=gdm.num_workers,\n",
    "        )\n",
    "ds = create_dataset(\n",
    "        dataset_path = gdm.dataset_path,\n",
    "        dataset_name  = gdm.dataset_name,\n",
    "        dataset_type = 'test',\n",
    "        transform = gdm.transform_dict['test'],\n",
    "        num_workers=gdm.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = runners[0].model.to(device)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "y=torch.tensor([],device=device)\n",
    "y_hat=torch.tensor([],device=device)\n",
    "preds = []\n",
    "targets = []\n",
    "files = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(gdm.test_dataloader()):\n",
    "        files.extend(data.file_id)\n",
    "        targets.append(data.y)\n",
    "        data = data.to(device)\n",
    "        y = torch.cat((y,data.y))\n",
    "        out = model(data)\n",
    "        preds.append(out.clone().detach().cpu())\n",
    "        label = torch.argmax(out, dim=1) \n",
    "        y_hat = torch.cat((y_hat,label))\n",
    "        correct += torch.sum(label == data.y)\n",
    "        total += data.y.shape[0]\n",
    "  \n",
    "y = y.clone().detach().cpu().numpy()\n",
    "y_hat = y_hat.clone().detach().cpu().numpy() \n",
    "preds_ = torch.cat(preds,dim=0) #.permute(0,2,1)\n",
    "targets_ = torch.cat(targets,dim=0)\n",
    "metrics = torchmetrics.classification.Accuracy(num_classes=runner.cfg.dataset.num_classes, task=\"multiclass\", top_k=1) \n",
    "\n",
    "acc = metrics(preds_, targets_).detach().cpu().numpy()\n",
    "print(acc)\n",
    "# return confusion_matrix_computed, y, y_hat, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2ind = {c:[] for c in ds.categories}\n",
    "for i,d in enumerate(ds):\n",
    "    \n",
    "    class2ind[d.label[0]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2ind = {d.file_id: i  for i, d in enumerate(ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(0, len(ds),(1,))\n",
    "# idx = [6]\n",
    "# idx = (name2ind['l_0011.mat'],)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "y1_hat = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ii in tqdm(range(1000)):\n",
    "        \n",
    "        data = ds[idx][0]\n",
    "        data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = model(data)\n",
    "        label = torch.argmax(out, dim=1)\n",
    "        y1_hat.append(label.cpu().item())\n",
    "        correct += torch.sum(label == data.y)\n",
    "        total += data.y.shape[0]\n",
    "    # loss = runner.loss_fn(y_hat, y)\n",
    "print(data.file_id)  \n",
    "print(data.label[0])\n",
    "print(correct/total)\n",
    "print([ds.categories[int(c)] for c in np.unique(np.array(y1_hat))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
