{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from datasets.dataset_factory import create_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"haraghi\"\n",
    "project = \"DGCNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ids = ['32r0ngy0','kt655slc']\n",
    "\n",
    "artifact_dirs = [WandbLogger.download_artifact(artifact=f\"haraghi/DGCNN/model-{run_id}:best\") for run_id in run_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [api.run(osp.join(entity, project, run_id)).config for run_id in run_ids]\n",
    "cfgs = [OmegaConf.create(config) for config in configs]\n",
    "cfg_files = []\n",
    "for cfg in cfgs:\n",
    "    if \"cfg_path\" in cfg.keys():\n",
    "        cfg_files.append(OmegaConf.load(cfg.cfg_path))\n",
    "    else:\n",
    "        cfg_files.append(cfg)\n",
    "            \n",
    "    \n",
    "# cfg = OmegaConf.merge(cfg_file, cfg)\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init()\n",
    "# checkpoint_reference = osp.join(entity, project, \"model-\" + run_id+\":best\")\n",
    "# artifact = run.use_artifact(checkpoint_reference, type='model')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_dict_compare(all_cfg, other_cfg):\n",
    "    \"\"\"\n",
    "    Recursively compare two dictionaries and return their differences.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    diff = {}\n",
    "\n",
    "    # Check for keys in dict1 that are not in dict2\n",
    "    for key in other_cfg:\n",
    "        if key not in all_cfg:\n",
    "            diff[key] = other_cfg[key]\n",
    "        else:\n",
    "            # If the values are dictionaries, recursively compare them\n",
    "            if isinstance(all_cfg[key], dict) and isinstance(other_cfg[key], dict):\n",
    "                nested_diff = recursive_dict_compare(all_cfg[key], other_cfg[key])\n",
    "                if nested_diff:\n",
    "                    diff[key] = nested_diff\n",
    "            # Otherwise, compare the values directly\n",
    "            elif all_cfg[key] != other_cfg[key]:\n",
    "                if not(key == \"num_classes\" and other_cfg[key] is None and all_cfg[key] is not None):\n",
    "                    diff[key] = other_cfg[key]\n",
    "                    \n",
    "\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print([recursive_dict_compare(OmegaConf.to_object(cfg),OmegaConf.to_object(cfg_file)) for cfg, cfg_file in zip(cfgs, cfg_files)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(t, q):\n",
    "    B, C, H, W = t.shape\n",
    "    k = 1 + round(.01 * float(q) * (C * H * W - 1))\n",
    "    result = t.view(B, -1).kthvalue(k).values\n",
    "    return result[:,None,None,None]\n",
    "\n",
    "def create_image(representation):\n",
    "    B, C, H, W = representation.shape\n",
    "    representation = representation.view(B, 3, C // 3, H, W).sum(2)\n",
    "\n",
    "    # do robust min max norm\n",
    "    representation = representation.detach().cpu()\n",
    "    robust_max_vals = percentile(representation, 99)\n",
    "    robust_min_vals = percentile(representation, 1)\n",
    "\n",
    "    representation = (representation - robust_min_vals)/(robust_max_vals - robust_min_vals)\n",
    "    representation = torch.clamp(255*representation, 0, 255).byte()\n",
    "\n",
    "    representation = torchvision.utils.make_grid(representation)\n",
    "\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything. Note that this does not make training entirely\n",
    "# deterministic.\n",
    "for cfg in cfgs:\n",
    "    pl.seed_everything(cfg.seed, workers=True)\n",
    "\n",
    "for cfg in cfgs[1:]:\n",
    "    compare_dict = recursive_dict_compare(OmegaConf.to_object(cfgs[0].dataset),OmegaConf.to_object(cfg.dataset))\n",
    "    if len(compare_dict)!=0:\n",
    "        if not (len(compare_dict) == 1 and 'num_workers' in compare_dict.keys()):\n",
    "            print(compare_dict)\n",
    "            print(cfg.dataset)\n",
    "            print(cfgs[0].dataset)\n",
    "            # raise Exception(\"Datasets are not the same\")\n",
    "# Create datasets using factory pattern\n",
    "gdm = GraphDataModule(cfgs[0])\n",
    "for cfg in cfgs:\n",
    "    cfg.dataset.num_classes = gdm.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file_list = []\n",
    "# for data in gdm.test_dataloader():\n",
    "#     test_file_list.extend(data.file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_list = []\n",
    "# for data in gdm.train_dataloader():\n",
    "#     train_file_list.extend(data.file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_file_list = []\n",
    "# for data in gdm.val_dataloader():\n",
    "#     val_file_list.extend(data.file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_file_list))\n",
    "# print(len(test_file_list))\n",
    "# print(len(val_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(set(test_file_list).intersection(set(val_file_list)))\n",
    "# print(set(test_file_list).intersection(set(train_file_list)))\n",
    "# print(set(val_file_list).intersection(set(train_file_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[2]\n",
    "saved_param = saved_params[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mm,nn in model.named_parameters():\n",
    "    print(mm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk in saved_param['state_dict'].keys():\n",
    "    print(kk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_params = []\n",
    "for artifact_dir, cfg, model in zip(artifact_dirs, cfgs, models):\n",
    "    print(artifact_dir)\n",
    "    saved_params.append(torch.load(osp.join(artifact_dir,\"model.ckpt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [model_factory.factory(cfg) for cfg in cfgs]\n",
    "\n",
    "# Tie it all together with PyTorch Lightning: Runner contains the model,\n",
    "# optimizer, loss function and metrics; Trainer executes the\n",
    "# training/validation loops and model checkpointing.\n",
    " \n",
    "runners = [Runner.load_from_checkpoint(osp.join(artifact_dir,\"model.ckpt\"), cfg=cfg, model=model) for artifact_dir, cfg, model in zip(artifact_dirs, cfgs, models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    enable_progress_bar=True,\n",
    "    # Use DDP training by default, even for CPU training\n",
    "    # strategy=\"ddp_notebook\",\n",
    "    devices=torch.cuda.device_count(),\n",
    "    accelerator=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runner in runners:\n",
    "    print(runner.cfg.model)\n",
    "    trainer.test(runner, datamodule=gdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "            create_dataset(\n",
    "                dataset_path = gdm.dataset_path,\n",
    "                dataset_name  = gdm.dataset_name,\n",
    "                dataset_type = 'test',\n",
    "                transform = gdm.transform_dict['test'],\n",
    "                num_workers=gdm.num_workers\n",
    "            ),\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=gdm.num_workers,\n",
    "        )\n",
    "ds = create_dataset(\n",
    "        dataset_path = gdm.dataset_path,\n",
    "        dataset_name  = gdm.dataset_name,\n",
    "        dataset_type = 'test',\n",
    "        transform = gdm.transform_dict['test'],\n",
    "        num_workers=gdm.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(runner,gdm):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = runner.model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    y=torch.tensor([],device=device)\n",
    "    y_hat=torch.tensor([],device=device)\n",
    "    preds = []\n",
    "    targets = []\n",
    "    files = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(gdm.test_dataloader()):\n",
    "            files.extend(data.file_id)\n",
    "            targets.append(data.y)\n",
    "            data = data.to(device)\n",
    "            y = torch.cat((y,data.y))\n",
    "            out = model(data)\n",
    "            preds.append(out.clone().detach().cpu())\n",
    "            label = torch.argmax(out, dim=1) \n",
    "            y_hat = torch.cat((y_hat,label))\n",
    "            correct += torch.sum(label == data.y)\n",
    "            total += data.y.shape[0]\n",
    "    print(correct/total)   \n",
    "    y = y.clone().detach().cpu().numpy()\n",
    "    y_hat = y_hat.clone().detach().cpu().numpy() \n",
    "    preds_ = torch.cat(preds,dim=0) #.permute(0,2,1)\n",
    "    targets_ = torch.cat(targets,dim=0)\n",
    "    metrics = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=runner.cfg.dataset.num_classes) \n",
    "    metrics.update(preds_, targets_)\n",
    "    confusion_matrix_computed = metrics.compute().detach().cpu().numpy().astype(int)\n",
    "    return confusion_matrix_computed, y, y_hat, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_outputs = [compute_confusion_matrix(runner,gdm) for runner in runners]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,CM_output in enumerate(CM_outputs):\n",
    "    print(cfgs[i].model)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=CM_output[0], display_labels = [f'{i}:{c}' for i,c in enumerate(ds.categories)])\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(16,14))\n",
    "    disp.plot(ax=axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_zero = confusion_matrix_computed - np.diag(np.diag(confusion_matrix_computed))\n",
    "gg =  np.unravel_index(np.argsort(off_zero, axis=None), off_zero.shape)\n",
    "for i in range(1, 20):\n",
    "    print(f'The {i}th largest off-diagonal element is {off_zero[gg[0][-i], gg[1][-i]]} at ({gg[0][-i]}:{ds.categories[gg[0][-i]]} -> {gg[1][-i]}:{ds.categories[gg[1][-i]]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_mat = np.diag(np.sum(off_zero,axis=1)) #+ np.diag(np.sum(off_zero,axis=0)) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=err_mat, display_labels = [f'{i}:{c}' for i,c in enumerate(ds.categories)])\n",
    "fig, axs = plt.subplots(1, 1, figsize=(16,14))\n",
    "disp.plot(ax=axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = CM_outputs[0][1]\n",
    "files = CM_outputs[0][3]\n",
    "y_mobilenet = CM_outputs[0][2]\n",
    "y_resnet = CM_outputs[1][2]\n",
    "\n",
    "study_list = []\n",
    "for i in np.where((y_true == 1) & (y_resnet == 1) & (y_mobilenet == 0))[0]:\n",
    "    study_list.append((i,osp.join(ds[i].label[0],files[i])))\n",
    "    print(i,ds[i].label[0],files[i])\n",
    "print(len(study_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2ind = {d.file_id: i  for i, d in enumerate(ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for n,v in name2ind.items():\n",
    "    if n.startswith('t_'):\n",
    "        print(n, v)\n",
    "        count += 1\n",
    "        if count == 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(0, len(ds),(1,))\n",
    "idx = [6]\n",
    "# idx = (name2ind['l_0011.mat'],)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "y1_hat = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ii in tqdm(range(1000)):\n",
    "        \n",
    "        data = ds[idx][0]\n",
    "        data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = model(data)\n",
    "        label = torch.argmax(out, dim=1)\n",
    "        y1_hat.append(label.cpu().item())\n",
    "        correct += torch.sum(label == data.y)\n",
    "        total += data.y.shape[0]\n",
    "    # loss = runner.loss_fn(y_hat, y)\n",
    "print(data.file_id)  \n",
    "print(data.label[0])\n",
    "print(correct/total)\n",
    "print([ds.categories[int(c)] for c in np.unique(np.array(y1_hat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(y1_hat) !=  ds[idx][0].y.clone().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4160\n",
    "# idx = name2ind['cars/obj_000623_td.dat']\n",
    "data = ds[idx]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "data = data.to(device)\n",
    "\n",
    "for model in models:\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    vox = model.quantization_layer.forward(data)\n",
    "    label = torch.argmax(out, dim=1)\n",
    "    rep = create_image(vox)\n",
    "    axs.imshow(rep.permute(1,2,0))\n",
    "    axs.invert_xaxis()\n",
    "    axs.set_title(f\"Predict: {label.item()}/{ds.categories[label.item()]}, file id: {data.label[0]}/{data.file_id}\")\n",
    "    axs.set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ys = [d.y.clone().detach().item() for d in ds]\n",
    "ys = np.array(ys)\n",
    "\n",
    "# count the number of happening of each value in the numpy array\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "\n",
    "print(counts)\n",
    "print(counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "m = 2\n",
    "with torch.no_grad():\n",
    "    for u in unique:\n",
    "        fig, axs = plt.subplots(n, m, figsize=(m*8,n*6))\n",
    "        if n == 1:\n",
    "            axs = axs[None,...]\n",
    "        if m == 1:\n",
    "            axs = axs[...,None]\n",
    "        print(u)\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                \n",
    "                indices = np.where(ys == u)[0]\n",
    "                idx = np.random.choice(indices)\n",
    "                \n",
    "                data = ds[idx]\n",
    "                \n",
    "                data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "                data = data.to(device)\n",
    "                out = model(data)\n",
    "                vox = model.quantization_layer.forward(data)\n",
    "                label = torch.argmax(out, dim=1)\n",
    "                if label != u:\n",
    "                    print(\"Wrong label\")\n",
    "                rep = create_image(vox)\n",
    "                axs[i,j].imshow(rep.permute(1,2,0))\n",
    "                # axs[i,j].invert_yaxis()\n",
    "                axs[i,j].set_title(f\"Predict: {label.item()}/{ds.categories[label.item()]}, file id: {data.label[0]}/{data.file_id}, idx: {idx}\")\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.categories[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
