{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = create_dataset(\n",
    "        dataset_name  = 'NASL',\n",
    "        dataset_type = 'all'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the sample indices for each class in a Dictionary\n",
    "# and create a mappings\n",
    "# The code takes a while to run!\n",
    "#\n",
    "# label2num: label -> number Example: 'a' -> 0\n",
    "# num2label: number -> label Example: 0 -> 'a'\n",
    "# name2ind: file_id -> sample index Example: 't_3803.mat' -> 17\n",
    "\n",
    "class_dict = {}\n",
    "num2label = {}\n",
    "name2ind = {}\n",
    "\n",
    "for idx,data in enumerate(ds):\n",
    "    y = data.y[0].item()\n",
    "    label = data.label[0]\n",
    "    file_id = data.file_id\n",
    "    \n",
    "    if label not in class_dict:\n",
    "        class_dict[label] = [idx]\n",
    "        num2label[y] = label\n",
    "    else:\n",
    "        class_dict[label].append(idx)\n",
    "    \n",
    "    \n",
    "    name2ind[file_id] = idx\n",
    "    \n",
    "label2num = {v: k for k, v in num2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of events per sample\n",
    "num_events = []\n",
    "for idx in class_dict[num2label[0]]:\n",
    "    num_events.append(ds[idx].num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu' # CPU is faster for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate the number of events for each class\n",
    "# without separating the polarities\n",
    "# \n",
    "# all_images: Dictionary with the accumulated events for each class\n",
    "\n",
    "all_images = {}\n",
    "\n",
    "for key,val in class_dict.items():\n",
    "\n",
    "    print(key)\n",
    "    img = torch.zeros([180, 240], dtype=torch.int64).to(device)\n",
    "\n",
    "\n",
    "    for idx in tqdm(val):\n",
    "        data = ds[idx].to(device)\n",
    "        x = data.pos[:,0].long()\n",
    "        y = data.pos[:,1].long()\n",
    "\n",
    "        img.index_put_((y, x), torch.ones_like(y).to(device), accumulate=True)\n",
    "    all_images[key] = img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the top K values and their indices\n",
    "\n",
    "def top_k_max_values_and_indices(img, K):\n",
    "    topk_values, topk_indices = torch.topk(img.view(-1), k=K)\n",
    "    topk_coords = torch.stack([topk_indices // img.shape[1],topk_indices % img.shape[1]], dim=-1)\n",
    "    topk_normalized_values = topk_values / torch.sum(img)\n",
    "    return topk_values, topk_coords, topk_normalized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the event number histogram for all classes together\n",
    "# \n",
    "# global_img: the accumulated events for all classes\n",
    "\n",
    "global_img = torch.zeros_like(all_images['a'],dtype=torch.long).to(device)\n",
    "for key,val in all_images.items():\n",
    "    global_img += val\n",
    "    \n",
    "fig = plt.figure(figsize=(14,9))\n",
    "plt.imshow(np.log(global_img.cpu().numpy()))\n",
    "plt.colorbar()\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K Global pixels having the most number of events\n",
    "\n",
    "\n",
    "K = 10\n",
    "\n",
    "global_K_val,global_K_idx, global_K_normalized_val = top_k_max_values_and_indices(global_img, K)\n",
    "# print([global_K_idx,global_K_val,global_K_normalized_val])\n",
    "print(global_K_idx[:,[1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K pixels having the most number of events for each class\n",
    "#\n",
    "# max_dict: Dictionary with the top K pixels for each class, their values and normalized values\n",
    "class_K = 5\n",
    "max_dict = {}\n",
    "for letter, img in all_images.items():\n",
    "    class_K_val, class_K_idx, class_K_normalized_val = top_k_max_values_and_indices(img, class_K)\n",
    "    max_dict[letter] = [class_K_idx, class_K_val, class_K_normalized_val]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the pixels and their values in a single tensor for all classes\n",
    "all_max = [torch.cat([v[0] for v in max_dict.values()], dim =0),\n",
    "              torch.cat([v[1] for v in max_dict.values()], dim =0),\n",
    "              torch.cat([v[2] for v in max_dict.values()], dim =0)]\n",
    "# Find the unique pixels\n",
    "_, unique_idx = np.unique(all_max[0], axis=0,return_index=True)\n",
    "all_max_unique = [v[unique_idx] for v in all_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sort(all_max_unique[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresh_normalized = 14e-5\n",
    "\n",
    "unique_idx_thresh_normalized = all_max_unique[2] > Thresh_normalized\n",
    "\n",
    "all_max_unique_thresh_normalized = [v[unique_idx_thresh_normalized] for v in all_max_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresh = 17000\n",
    "\n",
    "unique_idx_thresh = all_max_unique[1] > Thresh\n",
    "\n",
    "all_max_unique_thresh = [v[unique_idx_thresh] for v in all_max_unique]\n",
    "\n",
    "# for letter, max_mat in max_dict.items():\n",
    "#     print(letter)\n",
    "#     idx = max_mat[:,-1] > Thresh \n",
    "#     print(max_mat[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter, max_mat in max_dict.items():\n",
    "    print(letter)\n",
    "    img = torch.zeros([180, 240], dtype=torch.float).to(device)\n",
    "    img.index_put_((max_mat[0][:,0], max_mat[0][:,1]),torch.log(max_mat[1]) , accumulate=False)\n",
    "    plt.imshow(img.cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmove_pixels = all_max_unique_thresh_normalized[0]\n",
    "rmove_pixels = global_K_idx\n",
    "for letter, img_orig in all_images.items():\n",
    "    img = img_orig.detach().clone()\n",
    "    print(letter)\n",
    "    img.index_put_((rmove_pixels[:,0], rmove_pixels[:,1]),torch.zeros_like(rmove_pixels[:,0]) , accumulate=False)\n",
    "    plt.imshow(img.cpu().numpy())\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
