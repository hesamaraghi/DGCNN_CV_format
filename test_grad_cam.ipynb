{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"haraghi\"\n",
    "project = \"DGCNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ids = ['9rrxu350','x4lf35wy']\n",
    "# run_ids = ['2yqeh948']\n",
    "\n",
    "artifact_dirs = [WandbLogger.download_artifact(artifact=f\"{entity}/{project}/model-{run_id}:best\") for run_id in run_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_bare = OmegaConf.load(\"config_bare.yaml\")\n",
    "configs = [api.run(osp.join(entity, project, run_id)).config for run_id in run_ids]\n",
    "cfgs = [OmegaConf.merge(cfg_bare,OmegaConf.create(config)) for config in configs]\n",
    "cfg_files = []\n",
    "for cfg in cfgs:\n",
    "    if \"cfg_path\" in cfg.keys():\n",
    "        print(cfg.cfg_path)\n",
    "        cfg_files.append(OmegaConf.merge(cfg_bare,OmegaConf.load(cfg.cfg_path)))\n",
    "    else:\n",
    "        cfg_files.append(cfg)\n",
    "            \n",
    "    \n",
    "# cfg = OmegaConf.merge(cfg_file, cfg)\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_dict_compare(all_cfg, other_cfg):\n",
    "    \"\"\"\n",
    "    Recursively compare two dictionaries and return their differences.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    diff = {}\n",
    "\n",
    "    # Check for keys in dict1 that are not in dict2\n",
    "    for key in other_cfg:\n",
    "        if key not in all_cfg:\n",
    "            diff[key] = other_cfg[key]\n",
    "        else:\n",
    "            # If the values are dictionaries, recursively compare them\n",
    "            if isinstance(all_cfg[key], dict) and isinstance(other_cfg[key], dict):\n",
    "                nested_diff = recursive_dict_compare(all_cfg[key], other_cfg[key])\n",
    "                if nested_diff:\n",
    "                    diff[key] = nested_diff\n",
    "            # Otherwise, compare the values directly\n",
    "            elif all_cfg[key] != other_cfg[key]:\n",
    "                if not(key == \"num_classes\" and other_cfg[key] is None and all_cfg[key] is not None):\n",
    "                    diff[key] = other_cfg[key]\n",
    "                    \n",
    "\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([recursive_dict_compare(OmegaConf.to_object(cfg),OmegaConf.to_object(cfg_file)) for cfg, cfg_file in zip(cfgs, cfg_files)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything. Note that this does not make training entirely\n",
    "# deterministic.\n",
    "for cfg in cfgs:\n",
    "    pl.seed_everything(cfg.seed, workers=True)\n",
    "\n",
    "for cfg in cfgs[1:]:\n",
    "    compare_dict = recursive_dict_compare(OmegaConf.to_object(cfgs[0].dataset),OmegaConf.to_object(cfg.dataset))\n",
    "    if len(compare_dict)!=0:\n",
    "        if not (len(compare_dict) == 1 and 'num_workers' in compare_dict.keys()):\n",
    "            print(compare_dict)\n",
    "            print(cfg.dataset)\n",
    "            print(cfgs[0].dataset)\n",
    "            # raise Exception(\"Datasets are not the same\")\n",
    "# Create datasets using factory pattern\n",
    "\n",
    "\n",
    "gdm = GraphDataModule(cfgs[0])\n",
    "for cfg in cfgs:\n",
    "    cfg.dataset.num_classes = gdm.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(t, q):\n",
    "    B, C, H, W = t.shape\n",
    "    k = 1 + round(.01 * float(q) * (C * H * W - 1))\n",
    "    result = t.view(B, -1).kthvalue(k).values\n",
    "    return result[:,None,None,None]\n",
    "\n",
    "def create_image(representation):\n",
    "    B, C, H, W = representation.shape\n",
    "    representation = representation.view(B, 3, C // 3, H, W).sum(2)\n",
    "\n",
    "    # do robust min max norm\n",
    "    representation = representation.detach().cpu()\n",
    "    robust_max_vals = percentile(representation, 99)\n",
    "    robust_min_vals = percentile(representation, 1)\n",
    "\n",
    "    representation = (representation - robust_min_vals)/(robust_max_vals - robust_min_vals)\n",
    "    representation = torch.clamp(255*representation, 0, 255).byte()\n",
    "\n",
    "    representation = torchvision.utils.make_grid(representation)\n",
    "\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_factory.factory(cfg) for cfg in cfgs]\n",
    "\n",
    "# Tie it all together with PyTorch Lightning: Runner contains the model,\n",
    "# optimizer, loss function and metrics; Trainer executes the\n",
    "# training/validation loops and model checkpointing.\n",
    " \n",
    "runners = [Runner.load_from_checkpoint(osp.join(artifact_dir,\"model.ckpt\"), cfg=cfg, model=model) for artifact_dir, cfg, model in zip(artifact_dirs, cfgs, models)]\n",
    "\n",
    "gdms = [GraphDataModule(cfg) for cfg in cfgs]\n",
    "dss = []\n",
    "for cfg,gdm in zip(cfgs, gdms):\n",
    "    cfg.dataset.num_classes = gdm.num_classes\n",
    "\n",
    "    dss.append(create_dataset(\n",
    "        dataset_path = gdm.dataset_path,\n",
    "        dataset_name  = gdm.dataset_name,\n",
    "        dataset_type = 'test',\n",
    "        transform = gdm.transform_dict['test'],\n",
    "        num_workers=gdm.num_workers\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2ind = {c:[] for c in dss[0].categories}\n",
    "for i,d in enumerate(dss[0]):\n",
    "    \n",
    "    class2ind[d.label[0]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2ind.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = 'speed_3'\n",
    "idx = class2ind[class_name][torch.randint(0, len(class2ind[class_name]),(1,))]\n",
    "\n",
    "SPARSE  = 0\n",
    "DENSE = 1\n",
    "\n",
    "TEST = SPARSE\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = runners[TEST].model.to(device)\n",
    "\n",
    "target_layers = [model.classifier.layer4[-1]]\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=model.classifier, target_layers=target_layers, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dss[TEST][idx]\n",
    "data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "data = data.to(device)\n",
    "\n",
    "targets = [ClassifierOutputTarget(data.y.item())]\n",
    "\n",
    "vox = model.quantization_layer.forward(data)\n",
    "vox_cropped = model.crop_and_resize_to_resolution(vox, model.crop_dimension).clone().detach()\n",
    "\n",
    "out = model(data)\n",
    "pred_label = torch.argmax(out, dim=1) \n",
    "print('True label:', data.y.item(), 'Predicted label:', pred_label.item())\n",
    "\n",
    "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "grayscale_cam = cam(input_tensor=vox_cropped, targets=targets)\n",
    "\n",
    "# In this example grayscale_cam has only one image in the batch:\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "data_dense = dss[DENSE][idx]\n",
    "data_dense.batch = torch.zeros(data_dense.num_nodes, dtype=torch.long)\n",
    "data_dense = data_dense.to(device)\n",
    "\n",
    "vox = model.quantization_layer.forward(data_dense)\n",
    "vox_cropped = model.crop_and_resize_to_resolution(vox, model.crop_dimension)\n",
    "rep = create_image(vox_cropped)\n",
    "\n",
    "img = np.float32(rep.permute(1,2,0).detach().cpu().numpy())/255\n",
    "target_size = (img.shape[1], img.shape[0])  # (H, W)\n",
    "visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
    "plt.imshow(visualization)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "y=torch.tensor([],device=device)\n",
    "y_hat=torch.tensor([],device=device)\n",
    "preds = []\n",
    "targets = []\n",
    "files = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(dss[SPARSE]):\n",
    "        data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "        files.extend(data.file_id)\n",
    "        targets.append(data.y)\n",
    "        data = data.to(device)\n",
    "        y = torch.cat((y,data.y))\n",
    "        out = model(data)\n",
    "        preds.append(out.clone().detach().cpu())\n",
    "        label = torch.argmax(out, dim=1) \n",
    "        y_hat = torch.cat((y_hat,label))\n",
    "        correct += torch.sum(label == data.y)\n",
    "        total += data.y.shape[0]\n",
    "  \n",
    "y = y.clone().detach().cpu().numpy()\n",
    "y_hat = y_hat.clone().detach().cpu().numpy() \n",
    "preds_ = torch.cat(preds,dim=0) #.permute(0,2,1)\n",
    "targets_ = torch.cat(targets,dim=0)\n",
    "metrics = torchmetrics.classification.Accuracy(num_classes=runners[0].cfg.dataset.num_classes, task=\"multiclass\", top_k=1) \n",
    "\n",
    "acc = metrics(preds_, targets_).detach().cpu().numpy()\n",
    "print(acc)\n",
    "# return confusion_matrix_computed, y, y_hat, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "# run is specified by <entity>/<project>/<run_id>\n",
    "run = api.run(f\"haraghi/DGCNN/{run_ids[0]}\")\n",
    "\n",
    "# save the metrics for the run to a csv file\n",
    "metrics_dataframe = run.history()\n",
    "metrics_dataframe.to_csv(\"_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "# run is specified by <entity>/<project>/<run_id>\n",
    "run = api.run(f\"haraghi/DGCNN/{run_ids[0]}\")\n",
    "if run.state == \"finished\":\n",
    "    for i, row in run.history().iterrows():\n",
    "      print(row[\"_step\"], row[\"val/acc_step\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "# run = api.run(\"haraghi/DGCNN/<run_id>\")\n",
    "history = run.scan_history()\n",
    "losses = [row[\"train/loss_step\"] for row in history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "runs = api.runs(\"haraghi/DGCNN\")\n",
    "\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs: \n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files \n",
    "    summary_list.append(run.summary._json_dict)\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "          if not k.startswith('_')})\n",
    "\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)\n",
    "\n",
    "runs_df = pd.DataFrame({\n",
    "    \"summary\": summary_list,\n",
    "    \"config\": config_list,\n",
    "    \"name\": name_list\n",
    "    })\n",
    "\n",
    "runs_df.to_csv(\"project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs: \n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files \n",
    "    summary_list.append(run.summary._json_dict)\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "          if not k.startswith('_')})\n",
    "\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)\n",
    "\n",
    "\n",
    "\n",
    "runs_df.to_csv(\"project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configs = [api.run(osp.join(entity, project, run_id)).config for run_id in run_ids]\n",
    "cfgs = [OmegaConf.merge(cfg_bare,OmegaConf.create(config)) for config in configs]\n",
    "cfg_files = []\n",
    "for cfg in cfgs:\n",
    "    if \"cfg_path\" in cfg.keys():\n",
    "        print(cfg.cfg_path)\n",
    "        cfg_files.append(OmegaConf.merge(cfg_bare,OmegaConf.load(cfg.cfg_path)))\n",
    "    else:\n",
    "        cfg_files.append(cfg)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project is specified by <entity/project-name>\n",
    "runs = api.runs(\"haraghi/DGCNN\")\n",
    "cfg_bare = OmegaConf.load(\"config_bare.yaml\")\n",
    "cfgs = [OmegaConf.merge(cfg_bare,OmegaConf.create(run.config)) for run in runs]\n",
    "dataset_runs = {}\n",
    "# Get the dataset names from the config file\n",
    "dataset_names = list(set([cfg.dataset.name for cfg in cfgs]))\n",
    "for dataset_name in dataset_names:\n",
    "    # Get the runs for this dataset\n",
    "\n",
    "    dataset_runs[dataset_name] = [(run,cfg) for run,cfg in zip(runs,cfgs) if \n",
    "                                  cfg.dataset.name == dataset_name and \n",
    "                                  cfg.model.name == 'EST' and \n",
    "                                  cfg.model.num_bins == 9 and\n",
    "                                  cfg.model.resnet_pretrained and\n",
    "                                  'test/acc' in run.summary and\n",
    "                                  'epoch' in run.summary and\n",
    "                                  (not cfg.model.cnn_type or cfg.model.cnn_type == \"resnet34\") and\n",
    "                                  run.summary['epoch'] > 51 ]\n",
    "    \n",
    "\n",
    "    dataset_runs[dataset_name] = sorted(dataset_runs[dataset_name], key=lambda r: r[0].summary['test/acc'], reverse=True)\n",
    "    print(dataset_name, len(dataset_runs[dataset_name]))\n",
    "    # Get the run with the best validation accuracy\n",
    "    # best_run = sorted(dataset_runs, key=lambda r: r.summary['val/acc_step'], reverse=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run,cfg in dataset_runs['NCARS']:\n",
    "    artifact_dirs = [WandbLogger.download_artifact(artifact=f\"{entity}/{project}/model-{run_id}:best\") for run_id in run_ids]\n",
    "    print(run.summary['test/acc'], cfg.transform.train.num_events_per_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
