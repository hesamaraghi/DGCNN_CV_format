{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 90\n",
    "f2 = 110\n",
    "fs = 500\n",
    "n = np.arange(0, 100)\n",
    "n_unif = n[::1]\n",
    "N = 10000\n",
    "\n",
    "tensor_data = np.zeros((N,  len(n_unif),2 , 2))\n",
    "\n",
    "for s_idx in range(N):\n",
    "    rand_idx = np.random.permutation(len(n))[:len(n_unif)]\n",
    "    n_rand = n[rand_idx]\n",
    "    t_rand = n_rand / fs\n",
    "    # t_rand = (np.random.randn(len(n_rand))/3 +  n_rand)/fs\n",
    "    a_rand = np.cos(2 * np.pi * f1 * t_rand)\n",
    "    tensor_data[s_idx, :, 0, 0] = t_rand\n",
    "    tensor_data[s_idx, :, 1, 0] = a_rand\n",
    "    \n",
    "    rand_idx = np.random.permutation(len(n))[:len(n_unif)]\n",
    "    n_rand = n[rand_idx]\n",
    "    t_rand = n_rand / fs\n",
    "    # t_rand = (np.random.randn(len(n_rand))/3 +  n_rand)/fs\n",
    "    a_rand = np.cos(2 * np.pi * f2 * t_rand)\n",
    "    tensor_data[s_idx, :, 0, 1] = t_rand\n",
    "    tensor_data[s_idx, :, 1, 1] = a_rand\n",
    "\n",
    "# data_matrix = tensor_data.reshape((2,2 * len(n_unif), N))\n",
    "\n",
    "data_matrix = np.concatenate((tensor_data[...,0], tensor_data[...,1]), axis=0)\n",
    "labels = np.hstack((np.zeros(N), np.ones(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Convert data and labels to PyTorch tensors\n",
    "data = torch.tensor(data_matrix, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.long).squeeze()  # Assuming labels are in the shape (N, 1)\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader_graph = DataLoader([Data(x=x[:,1:2], pos=x[:,0:1],y=torch.tensor([y])) for x,y in zip(X_train, y_train)], batch_size=batch_size, shuffle=True)\n",
    "test_loader_graph = DataLoader([Data(x=x[:,1:2], pos=x[:,0:1],y=torch.tensor([y])) for x,y in zip(X_test, y_test)], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import MLP, EdgeConv, DynamicEdgeConv, global_max_pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,k):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.aggr = 'max'\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2 * 2,hidden_size,hidden_size]), self.k, self.aggr)\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2 * hidden_size,hidden_size,hidden_size]), self.k, self.aggr)\n",
    "        # self.conv2 = EdgeConv(MLP([2 * 64, 128]), aggr)\n",
    "        self.lin1 = Linear(hidden_size + hidden_size, 1024)\n",
    "\n",
    "        self.mlp = MLP([ 1024, 512, 256, output_size], dropout=0.5,\n",
    "                       batch_norm=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = data.pos\n",
    "        p = data.x\n",
    "        x0 = torch.cat([pos,p],dim=1)\n",
    "        if(data.batch is not None):\n",
    "            batch = data.batch\n",
    "            x1 = self.conv1(x0, batch)\n",
    "            x2 = self.conv2(x1, batch)\n",
    "            out1 = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "            out2 = global_max_pool(out1, batch)\n",
    "        else:       \n",
    "            x1 = self.conv1(x0)\n",
    "            x2 = self.conv2(x1)\n",
    "            out1 = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "            out2 = out1.sum(dim=-2, keepdim=out1.dim() == 2)\n",
    "        out = self.mlp(out2)\n",
    "        # inters = {'x1': x1, 'x2': x2, 'out1': out1,'out2': out2, 'out': out}\n",
    "        return F.log_softmax(out, dim=1) #, inters\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = np.prod(data.shape[1:])\n",
    "hidden_size = 64\n",
    "output_size = len(torch.unique(labels))\n",
    "model = GNN(hidden_size, output_size,k=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch_x in train_loader_graph:\n",
    "        batch_x = batch_x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_x.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == batch_x.y).sum().item()\n",
    "        total_samples += batch_x.y.size(0)\n",
    "\n",
    "    # Calculate and print training accuracy for the epoch\n",
    "    train_accuracy = (total_correct / total_samples) * 100\n",
    "    tqdm.write(f'Epoch [{epoch + 1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x in test_loader_graph:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_x.y.size(0)\n",
    "        correct += (predicted == batch_x.y).sum().item()\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (correct / total) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.fc15 = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc15(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "input_size = np.prod(data.shape[1:])\n",
    "hidden_size = 64\n",
    "output_size = len(torch.unique(labels))\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 400\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x.view(-1,input_size))\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "    # Calculate and print training accuracy for the epoch\n",
    "    train_accuracy = (total_correct / total_samples) * 100\n",
    "    tqdm.write(f'Epoch [{epoch + 1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x.view(-1,input_size))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (correct / total) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
