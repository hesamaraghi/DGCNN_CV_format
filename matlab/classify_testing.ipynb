{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 80\n",
    "f2 = 120\n",
    "fs = 500\n",
    "n = np.arange(0, 1000)\n",
    "n_unif = n[::5]\n",
    "N = 10_000\n",
    "\n",
    "tensor_data = np.zeros((N,  len(n_unif),2 , 2))\n",
    "\n",
    "for s_idx in range(N):\n",
    "    rand_idx = np.random.permutation(len(n))[:len(n_unif)]\n",
    "    n_rand = n[rand_idx]\n",
    "    n_rand = n_unif\n",
    "    t_rand = n_rand / fs\n",
    "    phi_rand = np.random.rand() * 2 * np.pi\n",
    "    # phi_rand = 0\n",
    "    # t_rand = (np.random.randn(len(n_rand))/3 +  n_rand)/fs\n",
    "    t_rand = (np.random.rand(len(n_rand)) * n[-1] )/fs\n",
    "    a_rand = np.cos(2 * np.pi * f1 * t_rand + phi_rand)\n",
    "    tensor_data[s_idx, :, 0, 0] = t_rand\n",
    "    tensor_data[s_idx, :, 1, 0] = a_rand\n",
    "    \n",
    "    rand_idx = np.random.permutation(len(n))[:len(n_unif)]\n",
    "    n_rand = n[rand_idx]\n",
    "    n_rand = n_unif\n",
    "    t_rand = n_rand / fs\n",
    "    phi_rand = np.random.rand() * 2 * np.pi\n",
    "    # phi_rand = 0\n",
    "    # t_rand = (np.random.randn(len(n_rand))/3 +  n_rand)/fs\n",
    "    t_rand = (np.random.rand(len(n_rand)) * n[-1] )/fs\n",
    "    a_rand = np.cos(2 * np.pi * f2 * t_rand + phi_rand)\n",
    "    tensor_data[s_idx, :, 0, 1] = t_rand\n",
    "    tensor_data[s_idx, :, 1, 1] = a_rand\n",
    "\n",
    "# data_matrix = tensor_data.reshape((2,2 * len(n_unif), N))\n",
    "\n",
    "data_matrix = np.concatenate((tensor_data[...,0], tensor_data[...,1]), axis=0)\n",
    "labels = np.hstack((np.zeros(N), np.ones(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.stem(tensor_data[0, :, 0, 0], tensor_data[0, :, 1, 0], use_line_collection=True)\n",
    "plt.show()\n",
    "plt.stem(tensor_data[0, :, 0, 1], tensor_data[0, :, 1, 1], use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm = []\n",
    "for d in data_matrix:\n",
    "    ttt = d[:,0]\n",
    "    aaa = d[:,1]\n",
    "    idx = np.argsort(ttt)\n",
    "    ttt = ttt[idx]\n",
    "    aaa = aaa[idx]\n",
    "\n",
    "    dif2 = ttt[2:] - ttt[:-2]\n",
    "    dif2[(np.abs(aaa[:-2] + aaa[1:-1] + aaa[2:]))/3<0.1] = 1e6\n",
    "\n",
    "    min_idx = np.argmin(dif2)\n",
    "    \n",
    "    \n",
    "    t0, t1, t2 = ttt[min_idx], ttt[min_idx+1], ttt[min_idx+2]\n",
    "    a0, a1, a2 = aaa[min_idx], aaa[min_idx+1], aaa[min_idx+2]\n",
    "    d1 = (a1 - a0) / (t1 - t0)\n",
    "    d2 = (a2 - a1) / (t2 - t1)\n",
    "    dd = (d2 - d1) / (t2 - t0) * 2\n",
    "    a_mean = (a0 + a1 + a2) / 3\n",
    "    mmm.append(-dd/4/np.pi/np.pi/(a_mean))\n",
    "    # mmm.append(a_mean)\n",
    "    \n",
    "thr = skimage.filters.threshold_multiotsu(image=np.array(mmm), classes=2)[0]    \n",
    "upper_class = np.zeros_like(mmm)\n",
    "upper_class[np.array(mmm) > thr] = 1\n",
    "print(np.sum(upper_class == labels) / len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample list\n",
    "data = np.array(mmm)\n",
    "# Plotting histogram\n",
    "plt.hist(data, bins=100, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of List')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample list\n",
    "data = np.array(mmm)\n",
    "# Plotting histogram\n",
    "data_upper = data[data > thr]\n",
    "plt.hist(data_upper, bins=100,color='r')\n",
    "data_lower = data[data < thr]\n",
    "plt.hist(data_lower, bins=100,color='b')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of List')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Convert data and labels to PyTorch tensors\n",
    "data = torch.tensor(data_matrix, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.long).squeeze()  # Assuming labels are in the shape (N, 1)\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader_graph = GraphDataLoader([Data(x=x[:,1:2], pos=x[:,0:1],y=torch.tensor([y])) for x,y in zip(X_train, y_train)], batch_size=batch_size, shuffle=True)\n",
    "test_loader_graph = GraphDataLoader([Data(x=x[:,1:2], pos=x[:,0:1],y=torch.tensor([y])) for x,y in zip(X_test, y_test)], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedTimeSeriesDataset(TensorDataset):\n",
    "    def __init__(self, tensors, n, fs):\n",
    "        super(QuantizedTimeSeriesDataset, self).__init__(*tensors)\n",
    "        self.time_bins = torch.tensor(n/fs)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = super(QuantizedTimeSeriesDataset, self).__getitem__(index)\n",
    "        time = data[0][:, 0]\n",
    "        amplitude = data[0][:, 1]\n",
    "\n",
    "        # Quantize the time into bins\n",
    "        quantized_time = torch.bucketize(time, self.time_bins) - 1\n",
    "\n",
    "        # Handle multiple amplitudes falling in the same time bin\n",
    "        binned_amplitude = torch.zeros(len(self.time_bins))\n",
    "        binned_amplitude.put_(quantized_time, amplitude, accumulate=False)               \n",
    "\n",
    "        return binned_amplitude, data[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return super(QuantizedTimeSeriesDataset, self).__len__()\n",
    "\n",
    "\n",
    "quantized_train_dataset = QuantizedTimeSeriesDataset([X_train, y_train], n, fs)\n",
    "train_loader = DataLoader(quantized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "quantized_test_dataset = QuantizedTimeSeriesDataset([X_test, y_test], n, fs)\n",
    "test_loader = DataLoader(quantized_test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "\n",
    "class TimeSeriesCNN(pl.LightningModule):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(TimeSeriesCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Calculate the size after four max-pooling layers\n",
    "        def calc_output_size(size, kernel_size=2, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        output_size = input_length\n",
    "        for _ in range(4):\n",
    "            output_size = calc_output_size(output_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Example usage\n",
    "input_length = len(n)  # Length of the quantized time series\n",
    "num_classes = 2  # Number of classes for classification\n",
    "model = TimeSeriesCNN(input_length, num_classes)\n",
    "\n",
    "\n",
    "# Set up W&B logger\n",
    "wandb_logger = WandbLogger(project='time_series_classification', log_model=False, reinit=True)\n",
    "\n",
    "# Disable model saving\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=0)\n",
    "\n",
    "# Train the model using PyTorch Lightning\n",
    "trainer = pl.Trainer(max_epochs=20, logger=wandb_logger, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderedTimeSeriesDataset(TensorDataset):\n",
    "    def __init__(self, tensors):\n",
    "        super(OrderedTimeSeriesDataset, self).__init__(*tensors)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = super(OrderedTimeSeriesDataset, self).__getitem__(index)\n",
    "        timestamps = data[0][:, 0]\n",
    "\n",
    "        ordered_time_index = torch.argsort(timestamps)\n",
    "        data[0][:,0] = data[0][ordered_time_index,0]\n",
    "        data[0][:,1] = data[0][ordered_time_index,1]\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return super(OrderedTimeSeriesDataset, self).__len__()\n",
    "\n",
    "\n",
    "ordered_train_dataset = OrderedTimeSeriesDataset([X_train, y_train])\n",
    "train_loader = DataLoader(ordered_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "ordered_test_dataset = OrderedTimeSeriesDataset([X_test, y_test])\n",
    "test_loader = DataLoader(ordered_test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "class TimeSeriesLSTM(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]  # Get the output of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Example usage\n",
    "input_dim = 2  # Number of features (time and amplitude)\n",
    "hidden_dim = 64    # Hidden dimension of LSTM\n",
    "num_classes = 2     # Number of classes for classification\n",
    "num_layers = 2\n",
    "model = TimeSeriesLSTM(input_dim, hidden_dim, num_classes, num_layers=num_layers)\n",
    "\n",
    "# Set up W&B logger\n",
    "wandb_logger = WandbLogger(project='time_series_lstm_classification', log_model=False, reinit=True)\n",
    "\n",
    "# Disable model saving\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=0)\n",
    "\n",
    "# Train the model using PyTorch Lightning\n",
    "trainer = pl.Trainer(max_epochs=10, logger=wandb_logger, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImplicitNN(nn.Module):\n",
    "    def __init__(self, mlp_layers, activation=nn.ReLU()):\n",
    "        assert mlp_layers[-1] == 1, \"Last layer of the mlp must have 1 input channel.\"\n",
    "        assert mlp_layers[0] == 1, \"First layer of the mlp must have 1 output channel\"\n",
    "\n",
    "        nn.Module.__init__(self)\n",
    "        self.mlp = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "\n",
    "        # create mlp\n",
    "        in_channels = 1\n",
    "        for out_channels in mlp_layers[1:]:\n",
    "            self.mlp.append(nn.Linear(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # init with trilinear kernel\n",
    "\n",
    "        # self.init_kernel(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # create sample of batchsize 1 and input channels 1\n",
    "        x = x[...,None]\n",
    "\n",
    "        # apply mlp convolution\n",
    "        for i in range(len(self.mlp[:-1])):\n",
    "            x = self.activation(self.mlp[i](x))\n",
    "\n",
    "        x = self.mlp[-1](x)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def init_kernel(self, num_channels):\n",
    "        ts = torch.zeros((1, 2000))\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        for _ in range(1000):  # converges in a reasonable time\n",
    "            optim.zero_grad()\n",
    "\n",
    "            ts.uniform_(-1, 1)\n",
    "\n",
    "            # gt\n",
    "            gt_values = self.trilinear_kernel(ts, num_channels)\n",
    "\n",
    "            # pred\n",
    "            values = self.forward(ts)\n",
    "\n",
    "            # optimize\n",
    "            loss = (values - gt_values).pow(2).sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "    def trilinear_kernel(self, ts, num_channels):\n",
    "        gt_values = torch.zeros_like(ts)\n",
    "\n",
    "        gt_values[ts > 0] = (1 - (num_channels-1) * ts)[ts > 0]\n",
    "        gt_values[ts < 0] = ((num_channels-1) * ts + 1)[ts < 0]\n",
    "\n",
    "        gt_values[ts < -1.0 / (num_channels-1)] = 0\n",
    "        gt_values[ts > 1.0 / (num_channels-1)] = 0\n",
    "\n",
    "        return gt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import MLP, EdgeConv, DynamicEdgeConv, global_max_pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,k):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.aggr = 'max'\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2 * 2,hidden_size,hidden_size]), self.k, self.aggr)\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2 * hidden_size,hidden_size,hidden_size]), self.k, self.aggr)\n",
    "        self.conv3 = DynamicEdgeConv(MLP([2 * hidden_size,hidden_size,hidden_size]), self.k, self.aggr)\n",
    "        # self.conv2 = EdgeConv(MLP([2 * 64, 128]), aggr)\n",
    "        self.lin1 = Linear(hidden_size + hidden_size, 1024)\n",
    "\n",
    "        self.mlp = MLP([ 1024, 512, 256, output_size], dropout=0.5,\n",
    "                       batch_norm=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = data.pos\n",
    "        p = data.x\n",
    "        x0 = torch.cat([pos,p],dim=1)\n",
    "        if(data.batch is not None):\n",
    "            batch = data.batch\n",
    "            x1 = self.conv1(x0, batch)\n",
    "            x2 = self.conv2(x1, batch)\n",
    "            # x2 = self.conv2(x2, batch)\n",
    "            out1 = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "            out2 = global_max_pool(out1, batch)\n",
    "        else:       \n",
    "            x1 = self.conv1(x0)\n",
    "            x2 = self.conv2(x1)\n",
    "            # x2 = self.conv2(x2)\n",
    "            out1 = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "            out2 = out1.sum(dim=-2, keepdim=out1.dim() == 2)\n",
    "        out = self.mlp(out2)\n",
    "        # inters = {'x1': x1, 'x2': x2, 'out1': out1,'out2': out2, 'out': out}\n",
    "        return F.log_softmax(out, dim=1) #, inters\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = np.prod(data.shape[1:])\n",
    "hidden_size = 64\n",
    "output_size = len(torch.unique(labels))\n",
    "model = GNN(hidden_size, output_size,k=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 150\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch_x in train_loader_graph:\n",
    "        batch_x = batch_x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_x.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == batch_x.y).sum().item()\n",
    "        total_samples += batch_x.y.size(0)\n",
    "\n",
    "    # Calculate and print training accuracy for the epoch\n",
    "    train_accuracy = (total_correct / total_samples) * 100\n",
    "    tqdm.write(f'Epoch [{epoch + 1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x in test_loader_graph:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_x.y.size(0)\n",
    "        correct += (predicted == batch_x.y).sum().item()\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (correct / total) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x in test_loader_graph:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_x.y.size(0)\n",
    "        correct += (predicted == batch_x.y).sum().item()\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (correct / total) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SineActivation, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sine = SineActivation()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.fc15 = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sine(x)\n",
    "        x = self.fc15(x)\n",
    "        x = self.sine(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "input_size = np.prod(data.shape[1:])\n",
    "hidden_size = 64\n",
    "output_size = len(torch.unique(labels))\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x.view(-1,input_size))\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "    # Calculate and print training accuracy for the epoch\n",
    "    train_accuracy = (total_correct / total_samples) * 100\n",
    "    tqdm.write(f'Epoch [{epoch + 1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x.view(-1,input_size))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (correct / total) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x.view(-1,input_size))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (correct / total) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
