{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"haraghi\"\n",
    "project = \"DGCNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ids = ['syifhzlv','1iyq2lum'] # 64 events, 20000 events\n",
    "# run_ids = ['icey2rjl','1iyq2lum'] # 8 events, 20000 events\n",
    "# run_ids = ['7zx0vpka','1iyq2lum'] # 8 events, 20000 events\n",
    "# run_ids = ['02o9q7aq','02o9q7aq'] # ShuffleNet 64 events\n",
    "# run_ids = ['9rrxu350','x4lf35wy']\n",
    "run_ids = ['9rrxu350','x4lf35wy'] # Fan data: 1024 events, 25000 events\n",
    "# \n",
    "SPARSE = 0\n",
    "DENSE = 1\n",
    "\n",
    "artifact_dirs = [WandbLogger.download_artifact(artifact=f\"{entity}/{project}/model-{run_id}:best\") for run_id in run_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_bare = OmegaConf.load(\"config_bare.yaml\")\n",
    "configs = [api.run(osp.join(entity, project, run_id)).config for run_id in run_ids]\n",
    "cfgs = [OmegaConf.merge(cfg_bare,OmegaConf.create(config)) for config in configs]\n",
    "cfg_files = []\n",
    "for cfg in cfgs:\n",
    "    if \"cfg_path\" in cfg.keys():\n",
    "        print(cfg.cfg_path)\n",
    "        cfg_files.append(OmegaConf.merge(cfg_bare,OmegaConf.load(cfg.cfg_path)))\n",
    "    else:\n",
    "        cfg_files.append(cfg)\n",
    "            \n",
    "    \n",
    "# cfg = OmegaConf.merge(cfg_file, cfg)\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_dict_compare(all_cfg, other_cfg):\n",
    "    \"\"\"\n",
    "    Recursively compare two dictionaries and return their differences.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    diff = {}\n",
    "\n",
    "    # Check for keys in dict1 that are not in dict2\n",
    "    for key in other_cfg:\n",
    "        if key not in all_cfg:\n",
    "            diff[key] = other_cfg[key]\n",
    "        else:\n",
    "            # If the values are dictionaries, recursively compare them\n",
    "            if isinstance(all_cfg[key], dict) and isinstance(other_cfg[key], dict):\n",
    "                nested_diff = recursive_dict_compare(all_cfg[key], other_cfg[key])\n",
    "                if nested_diff:\n",
    "                    diff[key] = nested_diff\n",
    "            # Otherwise, compare the values directly\n",
    "            elif all_cfg[key] != other_cfg[key]:\n",
    "                if not(key == \"num_classes\" and other_cfg[key] is None and all_cfg[key] is not None):\n",
    "                    diff[key] = other_cfg[key]\n",
    "                    \n",
    "\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([recursive_dict_compare(OmegaConf.to_object(cfg),OmegaConf.to_object(cfg_file)) for cfg, cfg_file in zip(cfgs, cfg_files)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything. Note that this does not make training entirely\n",
    "# deterministic.\n",
    "for cfg in cfgs:\n",
    "    pl.seed_everything(cfg.seed, workers=True)\n",
    "\n",
    "for cfg in cfgs[1:]:\n",
    "    compare_dict = recursive_dict_compare(OmegaConf.to_object(cfgs[0].dataset),OmegaConf.to_object(cfg.dataset))\n",
    "    if len(compare_dict)!=0:\n",
    "        if not (len(compare_dict) == 1 and 'num_workers' in compare_dict.keys()):\n",
    "            print(compare_dict)\n",
    "            print(cfg.dataset)\n",
    "            print(cfgs[0].dataset)\n",
    "            # raise Exception(\"Datasets are not the same\")\n",
    "# Create datasets using factory pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdms = [GraphDataModule(cfg) for cfg in cfgs]\n",
    "dss = []\n",
    "for cfg,gdm in zip(cfgs, gdms):\n",
    "    cfg.dataset.num_classes = gdm.num_classes\n",
    "\n",
    "    dss.append(create_dataset(\n",
    "        dataset_path = gdm.dataset_path,\n",
    "        dataset_name  = gdm.dataset_name,\n",
    "        dataset_type = 'test',\n",
    "        transform = gdm.transform_dict['test'],\n",
    "        num_workers=gdm.num_workers\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    def __init__(self, module, device=None):\n",
    "        # we are going to hook some model's layer (module here)\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.device = device\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        # when the module.forward() is executed, here we intercept its\n",
    "        # input and output. We are interested in the module's output.\n",
    "        self.features = output.clone()\n",
    "        if self.device is not None:\n",
    "            self.features = self.features.to(self.device)\n",
    "        self.features.requires_grad_(True)\n",
    "\n",
    "    def close(self):\n",
    "        # we must call this method to free memory resources\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models = [model_factory.factory(cfg) for cfg in cfgs]\n",
    "\n",
    "# Tie it all together with PyTorch Lightning: Runner contains the model,\n",
    "# optimizer, loss function and metrics; Trainer executes the\n",
    "# training/validation loops and model checkpointing.\n",
    " \n",
    "runners = [Runner.load_from_checkpoint(osp.join(artifact_dir,\"model.ckpt\"), cfg=cfg, model=model) for artifact_dir, cfg, model in zip(artifact_dirs, cfgs, models)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sparse = runners[SPARSE].model.to(device)\n",
    "feature_maps_sparse = [ SaveFeatures(model_sparse.classifier.relu,device=device),\n",
    "                        SaveFeatures(model_sparse.classifier.layer1[-1],device=device),\n",
    "                        SaveFeatures(model_sparse.classifier.layer2[-1],device=device),\n",
    "                        SaveFeatures(model_sparse.classifier.layer3[-1],device=device),\n",
    "                        SaveFeatures(model_sparse.classifier.layer4[-1],device=device)]\n",
    "\n",
    "model_dense = runners[DENSE].model.to(device)\n",
    "feature_maps_dense = [ SaveFeatures(model_dense.classifier.relu,device=device),    \n",
    "                        SaveFeatures(model_dense.classifier.layer1[-1],device=device),\n",
    "                        SaveFeatures(model_dense.classifier.layer2[-1],device=device),\n",
    "                        SaveFeatures(model_dense.classifier.layer3[-1],device=device),\n",
    "                        SaveFeatures(model_dense.classifier.layer4[-1],device=device)]\n",
    "\n",
    "\n",
    "# For shuffleNet\n",
    "# model.classifier.conv1.register_forward_hook(hook_fn)\n",
    "# model.classifier.stage2[-1].register_forward_hook(hook_fn)\n",
    "# model.classifier.stage3[-1].register_forward_hook(hook_fn)\n",
    "# model.classifier.stage4[-1].register_forward_hook(hook_fn)\n",
    "# model.classifier.conv5.register_forward_hook(hook_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "idx = np.random.randint(len(dss[SPARSE]))\n",
    "# idx = 10799\n",
    "\n",
    "data = dss[SPARSE][idx]\n",
    "data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "data = data.to(device)\n",
    "\n",
    "model_sparse.eval()\n",
    "with torch.no_grad():\n",
    "    model_sparse(data)\n",
    "    \n",
    "# Visualize the sparse input\n",
    "vox_sparse = model_sparse.quantization_layer.forward(data)\n",
    "vox_cropped_sparse = model_sparse.crop_and_resize_to_resolution(vox_sparse, model_sparse.crop_dimension)\n",
    "vox_sparse = vox_sparse.clone().detach().cpu().numpy()\n",
    "vox_cropped_sparse = vox_cropped_sparse.clone().detach().cpu().numpy()\n",
    "\n",
    "print('voxel grid: ', vox_sparse[0].shape)\n",
    "print('sparse input with 64 events per sample',flush=True)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(np.min([vox_sparse[0].shape[0],256])):\n",
    "    plt.subplot(9, 9, i + 1)\n",
    "    plt.imshow(vox_sparse[0][i], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('voxel cropped grid: ', vox_cropped_sparse[0].shape)\n",
    "print('sparse input with 64 events per sample',flush=True)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(np.min([vox_cropped_sparse[0].shape[0],256])):\n",
    "    plt.subplot(9, 9, i + 1)\n",
    "    plt.imshow(vox_cropped_sparse[0][i], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "data = dss[DENSE][idx]\n",
    "data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "data = data.to(device)\n",
    "\n",
    "model_dense.eval()\n",
    "with torch.no_grad():\n",
    "    model_dense(data)\n",
    "\n",
    "# Visualize the dense input\n",
    "vox_dense = model_dense.quantization_layer.forward(data)\n",
    "vox_cropped_dense = model_dense.crop_and_resize_to_resolution(vox_dense, model_dense.crop_dimension)\n",
    "vox_dense = vox_dense.clone().detach().cpu().numpy()\n",
    "vox_cropped_dense = vox_cropped_dense.clone().detach().cpu().numpy()\n",
    "\n",
    "print('voxel grid: ', vox_sparse[0].shape)\n",
    "print('dense input with 20000 events per sample',flush=True)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(np.min([vox_dense[0].shape[0],256])):\n",
    "    plt.subplot(9, 9, i + 1)\n",
    "    plt.imshow(vox_dense[0][i], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('voxel cropped grid: ', vox_cropped_dense[0].shape)\n",
    "print('dense input with 20000 events per sample',flush=True)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(np.min([vox_cropped_dense[0].shape[0],256])):\n",
    "    plt.subplot(9, 9, i + 1)\n",
    "    plt.imshow(vox_cropped_dense[0][i], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "for number, fm in enumerate(feature_maps_sparse):\n",
    "    print('sparse input with 64 events per sample',flush=True)\n",
    "    \n",
    "    feature_map_sparse = fm.features.clone().detach().cpu().numpy()\n",
    "    print(number,feature_map_sparse[0].shape)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(np.min([feature_map_sparse[0].shape[0],48])):\n",
    "        plt.subplot(6, 8, i + 1)\n",
    "        plt.imshow(feature_map_sparse[0][i], cmap='viridis')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print('dense input with 20000 events per sample',flush=True)\n",
    "    feature_map_dense = feature_maps_dense[number].features.clone().detach().cpu().numpy()\n",
    "    print(number,feature_map_dense[0].shape)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i in range(np.min([feature_map_dense[0].shape[0],48])):\n",
    "        plt.subplot(6, 8, i + 1)\n",
    "        plt.imshow(feature_map_dense[0][i], cmap='viridis')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vox_dense \n",
    "# vox_cropped_dense\n",
    "# feature_maps_dense\n",
    "\n",
    "# tensor_1 = vox_sparse\n",
    "# tensor_2 = vox_dense\n",
    "\n",
    "tensor_1 = vox_cropped_sparse\n",
    "tensor_2 = vox_cropped_dense\n",
    "\n",
    "# tensor_1 = feature_maps_sparse[1].features.clone().detach().cpu().numpy()\n",
    "# tensor_2 = feature_maps_dense[1].features.clone().detach().cpu().numpy()\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "print(tensor_1.shape)\n",
    "print(tensor_2.shape)\n",
    "for i in np.arange(9):#range(np.min([tensor_1[0].shape[0],20])):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    print(i,'sparse')\n",
    "    img = tensor_1[0,i,...] + tensor_1[0,i+9,...]\n",
    "    # plt.imshow(np.concatenate([tensor_1[0,i,...],tensor_2[0,i,...]],axis = 1), cmap='binary')\n",
    "    plt.imshow(img, cmap='binary')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    # plt.show()\n",
    "    # print(i,'dense')\n",
    "    img = tensor_2[0,i,...] + tensor_2[0,i+9,...]\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img, cmap='binary')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = vox_sparse\n",
    "save_folder = os.path.join('images','feature_maps','fan')\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(((vox_cropped_sparse[0,0,...])+1e-6), cmap='viridis')\n",
    "orig_size = vox_cropped_sparse.shape[2]\n",
    "print(orig_size)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_sparse_0.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "tensor = feature_maps_sparse[0].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_sparse_1.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_sparse[1].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_sparse_2.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_sparse[2].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4 , i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_sparse_3.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_sparse[3].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5 , i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_sparse_4.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_sparse[4].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(36):\n",
    "    plt.subplot(6,6 , i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.pcolor(tensor[0,i,...], cmap='viridis', vmin=0, vmax=1)\n",
    "    # plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_sparse_5.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = vox_dense\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(((vox_cropped_dense[0,0,...])+1e-6), cmap='viridis')\n",
    "orig_size = vox_cropped_dense.shape[2]\n",
    "print(orig_size)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_dense_0.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tensor = feature_maps_dense[0].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_dense_1.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_dense[1].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_dense_2.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_dense[2].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4 , i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_dense_3.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_dense[3].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5 , i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_dense_4.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "tensor = feature_maps_dense[4].features.clone().detach().cpu().numpy()\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(36):\n",
    "    plt.subplot(6,6 , i+ 1)\n",
    "    tensor[0,i,...] = tensor[0,i,...]/(np.max(tensor[0,i,...], axis=None)+1e-8)\n",
    "    plt.pcolor(tensor[0,i,...], cmap='viridis', vmin=0, vmax=1)\n",
    "    # plt.imshow(tensor[0,i,...], cmap='viridis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "plt.savefig(os.path.join(save_folder,'nasl_dense_5.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners[0].model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in runners[0].model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_dense.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "y=torch.tensor([],device=device)\n",
    "y_hat=torch.tensor([],device=device)\n",
    "preds = []\n",
    "targets = []\n",
    "files = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(dss[SPARSE]):\n",
    "        data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "        files.extend(data.file_id)\n",
    "        targets.append(data.y)\n",
    "        data = data.to(device)\n",
    "        y = torch.cat((y,data.y))\n",
    "        out = model_dense(data)\n",
    "        preds.append(out.clone().detach().cpu())\n",
    "        label = torch.argmax(out, dim=1) \n",
    "        y_hat = torch.cat((y_hat,label))\n",
    "        correct += torch.sum(label == data.y)\n",
    "        total += data.y.shape[0]\n",
    "  \n",
    "y = y.clone().detach().cpu().numpy()\n",
    "y_hat = y_hat.clone().detach().cpu().numpy() \n",
    "preds_ = torch.cat(preds,dim=0) #.permute(0,2,1)\n",
    "targets_ = torch.cat(targets,dim=0)\n",
    "metrics = torchmetrics.classification.Accuracy(num_classes=runners[0].cfg.dataset.num_classes, task=\"multiclass\", top_k=1) \n",
    "\n",
    "acc = metrics(preds_, targets_).detach().cpu().numpy()\n",
    "print(acc)\n",
    "# return confusion_matrix_computed, y, y_hat, files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
