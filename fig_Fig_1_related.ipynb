{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "from transform_factory import factory as transforms\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from models.est import create_image\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from utils.config_utils import get_checkpoint_file, get_config_file, show_cfg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from models.est import create_image\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = api.projects(entity=\"haraghi\")\n",
    "for project in projects:\n",
    "    print(project.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name_and_num_classes = {\n",
    "    \"NCARS\": {\"name\": \"N-Cars\", \"num_classes\": 2},\n",
    "    \"NASL\": {\"name\": \"N-ASL\", \"num_classes\": 24},\n",
    "    \"NCALTECH101\": {\"name\": \"N-Caltech101\", \"num_classes\": 101},\n",
    "    \"DVSGESTURE_TONIC\": {\"name\": \"DVS-Gesture\", \"num_classes\": 11},\n",
    "    \"FAN1VS3\": {\"name\": \"Fan1vs3\", \"num_classes\": 2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_projects = [\n",
    "        \"FINAL-NASL-varyinig-sparsity\",\n",
    "        \"FINAL-NCARS-varyinig-sparsity\",\n",
    "        \"FINAL-DVSGESTURE_TONIC-HP-varyinig-sparsity\",\n",
    "        \"FINAL-FAN1vs3-varyinig-sparsity\",\n",
    "        \"FINAL-NCALTECH101-varyinig-sparsity\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_val_and_test_acc_keys(run):\n",
    "    val_acc_key = []\n",
    "    test_acc_key = []\n",
    "    for key in run.summary.keys():\n",
    "        if \"val\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            val_acc_key.append(key)\n",
    "        if \"test\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            test_acc_key.append(key)\n",
    "    assert len(val_acc_key) <= 1, f\"More than one val acc key found: {val_acc_key}\"\n",
    "    assert len(test_acc_key) <= 1, f\"More than one test acc key found: {test_acc_key}\"\n",
    "    return val_acc_key[0] if len(val_acc_key) == 1 else None , test_acc_key[0] if len(test_acc_key) == 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'paper'\n",
    "subfolder_name = os.path.join('images',folder_name,'fig_1')\n",
    "entity = 'haraghi'\n",
    "if not os.path.exists(subfolder_name):\n",
    "    os.makedirs(subfolder_name)\n",
    "\n",
    "test_dict = {}\n",
    "num_events_set = set() \n",
    "\n",
    "for project_name in dataset_projects:\n",
    "    runs = api.runs(f\"{entity}/{project_name}\")\n",
    "    runs = [r for r in runs if r.state == \"finished\" and \"transform\" in r.config]\n",
    "    if len(runs) == 0:\n",
    "        print(f\"No runs found for {project_name}\")\n",
    "        continue\n",
    "    num_events = np.unique([run.config['transform']['train']['num_events_per_sample'] for run in runs])\n",
    "    runs_per_num_events = {num_event: [run for run in runs if run.config['transform']['train']['num_events_per_sample'] == num_event] for num_event in num_events}\n",
    "    dataset_name = runs[0].config[\"dataset\"][\"name\"]\n",
    "    \n",
    "    num_events_set = num_events_set.union(set(num_events))\n",
    "    \n",
    "    test_mean = {}\n",
    "    test_max = {}\n",
    "    for num_event in num_events:\n",
    "        test_mean[num_event] = []\n",
    "        tes_max_val = -1\n",
    "        for run in runs_per_num_events[num_event]:\n",
    "            _, test_key = find_val_and_test_acc_keys(run)\n",
    "            if test_key in run.summary: \n",
    "                test_mean[num_event].append(run.summary[test_key])\n",
    "                if run.summary[test_key] > tes_max_val:\n",
    "                    tes_max_val = run.summary[test_key]\n",
    "                    test_max[num_event] = (run.summary[test_key], run)\n",
    "            else:\n",
    "                test_mean[num_event].append(None)\n",
    "            \n",
    "        print(f\"percentage of runs with test acc for {num_event} events: {np.sum([v is not None for v in test_mean[num_event]]) / len(test_mean[num_event])} out of {len(test_mean[num_event])} runs\")\n",
    "    \n",
    "    test_dict[dataset_name] = [test_mean  ,test_max]\n",
    "\n",
    "num_events_list = sorted(list(num_events_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for dataset_name, test_results in test_dict.items(): \n",
    "    print(f\"Dataset: {dataset_name}\", flush=True)\n",
    "    if dataset_name == \"NASL\":\n",
    "        datasets[dataset_name] =   create_dataset(\n",
    "            dataset_path = os.path.join(\"datasets_torch_geometric\", dataset_name, 'data'),\n",
    "            dataset_name  = dataset_name,\n",
    "            dataset_type = 'test',\n",
    "            transform = None,#transforms(cfg.transform.train),\n",
    "            pre_transform = None,# transforms(cfg.pre_transform.train),\n",
    "            num_workers=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids =[13749,\n",
    "13217,\n",
    "14728,\n",
    "5696,\n",
    "1807,\n",
    "3742,\n",
    "3432]\n",
    "\n",
    "c =[ 1428, 1670]\n",
    "v = [12827, 13211]\n",
    "w = [13520, 13297, 13785]\n",
    "sample_ids = c + v + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_EST = True\n",
    "class_label = 'w'\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create custom colormaps\n",
    "cmap_blue = mcolors.LinearSegmentedColormap.from_list(\"blue_cmap\", [\"white\", \"blue\"])\n",
    "cmap_red = mcolors.LinearSegmentedColormap.from_list(\"red_cmap\", [\"white\", \"red\"])\n",
    "            \n",
    "for _ in range(30):\n",
    "# for sample_id in sample_ids:\n",
    "    for dataset_name, test_results in test_dict.items(): \n",
    "        dataset =  datasets[dataset_name]\n",
    "        while True:\n",
    "            sample_id = np.random.randint(len(dataset))\n",
    "            if dataset[sample_id].label[0].lower() == class_label.lower():\n",
    "                break\n",
    "        print(f\"sample_id: {sample_id} class: {dataset[sample_id].label[0]}\")\n",
    "        for num_event, test_max_run in test_results[1].items():\n",
    "            if num_event <10000:\n",
    "                continue\n",
    "            print(num_event)\n",
    "            run = test_max_run[1]     \n",
    "            cfg,_ = get_config_file(run.entity, run.project, run.id, verbose=False)\n",
    "            \n",
    "            H, W = cfg.dataset.image_resolution\n",
    "            \n",
    "            dataset.transform = transforms(cfg.transform.test) \n",
    "            data = dataset[sample_id]\n",
    "            vox = torch.zeros(2*H*W)\n",
    "            # get values for each channel\n",
    "            x, y, p = data.pos[:,0], data.pos[:,1], data.x[:,0]\n",
    "            p = (p+1)/2  # maps polarity to 0, 1\n",
    "            idx =         x.int() \\\n",
    "                        + W * y.int()\\\n",
    "                        + W * H * p.int()\n",
    "            vox.put_(idx.long(), vox.new_full([data.num_nodes,], fill_value=1), accumulate=True)\n",
    "            vox = vox.view(2, H, W)\n",
    "            vox_sum = vox.sum(0)\n",
    "            fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "            \n",
    "            # Choose a colormap\n",
    "            # cmap = plt.cm.viridis\n",
    "            # nonzero_indices = torch.nonzero(vox_sum, as_tuple=True)\n",
    "            # x_coords = nonzero_indices[0].numpy()\n",
    "            # y_coords = nonzero_indices[1].numpy()\n",
    "            # values = vox_sum[x_coords, y_coords]\n",
    "            # norm = (values - values.min()) / (values.max() - values.min())\n",
    "            # colors = cmap(norm.numpy())\n",
    "            # scatter_ax = ax[0].scatter(y_coords, x_coords, c=colors, s=np.ones_like(x_coords)*1, alpha=1.0)\n",
    "            \n",
    "            nonzero_indices = torch.nonzero(vox[0], as_tuple=True)\n",
    "            x_coords = nonzero_indices[0].numpy()\n",
    "            y_coords = nonzero_indices[1].numpy()\n",
    "            values = vox[0,x_coords, y_coords]\n",
    "            norm = (values - values.min()) / (values.max() - values.min())\n",
    "            colors = cmap_blue(norm.numpy())\n",
    "            scatter_ax = ax[0].scatter(y_coords, x_coords, c=colors, s=np.ones_like(x_coords)*1, alpha=.7)\n",
    "            cbar_blue = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=values.min(), vmax=values.max()), cmap=cmap_blue)\n",
    "            \n",
    "            \n",
    "            nonzero_indices = torch.nonzero(vox[1], as_tuple=True)\n",
    "            x_coords = nonzero_indices[0].numpy()\n",
    "            y_coords = nonzero_indices[1].numpy()\n",
    "            values = vox[1,x_coords, y_coords]\n",
    "            norm = (values - values.min()) / (values.max() - values.min())\n",
    "            colors = cmap_red(norm.numpy())\n",
    "            scatter_ax_2 = ax[0].scatter(y_coords, x_coords, c=colors, s=np.ones_like(x_coords)*1, alpha=.7)\n",
    "            cbar = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=values.min(), vmax=values.max()), cmap=cmap_red)\n",
    "            # cbar.set_label('Value')\n",
    "\n",
    "            ax[0].set_aspect('equal','box')\n",
    "            ax[0].set_xlim(-0.5, vox_sum.shape[1] - 0.5)\n",
    "            ax[0].set_ylim(-0.5, vox_sum.shape[0] - 0.5)\n",
    "            \n",
    "            \n",
    "            accumulate_ax = ax[1].imshow(vox_sum)\n",
    "            ax[1].set_aspect('equal','box')\n",
    "            ax[1].invert_yaxis()\n",
    "\n",
    "            if after_EST:\n",
    "                model = model_factory.factory(cfg).to(device)\n",
    "                checkpoint_file = get_checkpoint_file(run.entity, run.project, run.id)\n",
    "                runner = Runner.load_from_checkpoint(checkpoint_path=checkpoint_file, cfg=cfg, model=model, map_location=device)\n",
    "                data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "                data = data.to(device)\n",
    "                runner.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    vox_after_est = runner.model.quantization_layer.forward(data)\n",
    "                vox_after_est = create_image(vox_after_est)\n",
    "                est_ax = ax[2].imshow(vox_after_est.numpy().transpose(1,2,0), cmap='viridis')\n",
    "                ax[2].invert_yaxis()\n",
    "            # Show plot\n",
    "            ax_names = [cbar, accumulate_ax, est_ax]\n",
    "            for i in range(3):\n",
    "                ax[i].axis('off')\n",
    "                        # Remove labels and ticks\n",
    "                ax[i].set_xticks([])\n",
    "                ax[i].set_yticks([])\n",
    "                ax[i].set_xlabel('')\n",
    "                ax[i].set_ylabel('')\n",
    "                ax[i].set_title('')\n",
    "                ax[i].spines['top'].set_linewidth(2.0)\n",
    "                ax[i].spines['right'].set_linewidth(2.0)\n",
    "                ax[i].spines['bottom'].set_linewidth(2.0)\n",
    "                ax[i].spines['left'].set_linewidth(2.0)\n",
    "            \n",
    "                fig.colorbar(ax_names[i], ax=ax[i])\n",
    "            plt.show()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
