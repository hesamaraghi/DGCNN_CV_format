{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "from transform_factory import factory as transforms\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from utils.config_utils import get_checkpoint_file, get_config_file, show_cfg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = api.projects(entity=\"haraghi\")\n",
    "for project in projects:\n",
    "    print(project.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name_and_num_classes = {\n",
    "    \"NCARS\": {\"name\": \"N-Cars\", \"num_classes\": 2},\n",
    "    \"NASL\": {\"name\": \"N-ASL\", \"num_classes\": 24},\n",
    "    \"NCALTECH101\": {\"name\": \"N-Caltech101\", \"num_classes\": 101},\n",
    "    \"DVSGESTURE_TONIC\": {\"name\": \"DVS-Gesture\", \"num_classes\": 11},\n",
    "    \"FAN1VS3\": {\"name\": \"Fan1vs3\", \"num_classes\": 2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_projects = [\n",
    "        \"FINAL-NASL-varyinig-sparsity\",\n",
    "        \"FINAL-NCARS-varyinig-sparsity\",\n",
    "        \"FINAL-DVSGESTURE_TONIC-HP-varyinig-sparsity\",\n",
    "        \"FINAL-FAN1VS3-varyinig-sparsity\",\n",
    "        \"FINAL-NCALTECH101-varyinig-sparsity\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_val_and_test_acc_keys(run):\n",
    "    val_acc_key = []\n",
    "    test_acc_key = []\n",
    "    for key in run.summary.keys():\n",
    "        if \"val\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            val_acc_key.append(key)\n",
    "        if \"test\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            test_acc_key.append(key)\n",
    "    assert len(val_acc_key) <= 1, f\"More than one val acc key found: {val_acc_key}\"\n",
    "    assert len(test_acc_key) <= 1, f\"More than one test acc key found: {test_acc_key}\"\n",
    "    return val_acc_key[0] if len(val_acc_key) == 1 else None , test_acc_key[0] if len(test_acc_key) == 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'paper'\n",
    "subfolder_name = os.path.join('images',folder_name,'fig_1')\n",
    "entity = 'haraghi'\n",
    "if not os.path.exists(subfolder_name):\n",
    "    os.makedirs(subfolder_name)\n",
    "\n",
    "test_dict = {}\n",
    "num_events_set = set() \n",
    "\n",
    "for project_name in dataset_projects:\n",
    "    runs = api.runs(f\"{entity}/{project_name}\")\n",
    "    runs = [r for r in runs if r.state == \"finished\" and \"transform\" in r.config]\n",
    "    if len(runs) == 0:\n",
    "        print(f\"No runs found for {project_name}\")\n",
    "        continue\n",
    "    num_events = np.unique([run.config['transform']['train']['num_events_per_sample'] for run in runs])\n",
    "    runs_per_num_events = {num_event: [run for run in runs if run.config['transform']['train']['num_events_per_sample'] == num_event] for num_event in num_events}\n",
    "    dataset_name = runs[0].config[\"dataset\"][\"name\"]\n",
    "    \n",
    "    num_events_set = num_events_set.union(set(num_events))\n",
    "    \n",
    "    test_mean = {}\n",
    "    test_max = {}\n",
    "    for num_event in num_events:\n",
    "        test_mean[num_event] = []\n",
    "        tes_max_val = -1\n",
    "        for run in runs_per_num_events[num_event]:\n",
    "            _, test_key = find_val_and_test_acc_keys(run)\n",
    "            if test_key in run.summary: \n",
    "                test_mean[num_event].append(run.summary[test_key])\n",
    "                if run.summary[test_key] > tes_max_val:\n",
    "                    tes_max_val = run.summary[test_key]\n",
    "                    test_max[num_event] = (run.summary[test_key], run)\n",
    "            else:\n",
    "                test_mean[num_event].append(None)\n",
    "            \n",
    "        print(f\"percentage of runs with test acc for {num_event} events: {np.sum([v is not None for v in test_mean[num_event]]) / len(test_mean[num_event])} out of {len(test_mean[num_event])} runs\")\n",
    "    \n",
    "    test_dict[dataset_name] = [test_mean  ,test_max]\n",
    "\n",
    "num_events_list = sorted(list(num_events_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, test_results in test_dict.items():\n",
    "    \n",
    "    for num_event, test_max_run in test_results[1].items():\n",
    "        run = test_max_run[1]\n",
    "        cfg_file,_ = get_config_file(run.entity, run.project, run.id)\n",
    "        fdsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# experiment = \"non-trained\"\n",
    "# experiment = \"fully trained\"\n",
    "experiment = \"5 epoch trained\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if experiment == \"non-trained\":\n",
    "    sparse_path = \"haraghi/sweep EST (FAN1VS3) (multi val test num 20)/bnq6i70m\"\n",
    "    dense_path = \"haraghi/sweep EST (FAN1VS3) 25000 (multi val test num 20)/no47hfm9\"\n",
    "    cfg_sparse,_ = get_config_file(*sparse_path.split(\"/\"))\n",
    "    cfg_dense,_ = get_config_file(*dense_path.split(\"/\"))\n",
    "\n",
    "    model_sparse = model_factory.factory(cfg_sparse)\n",
    "    runner_sparse = Runner(cfg=cfg_sparse, model=model_sparse).to(device)\n",
    "    runner_dense = Runner(cfg=cfg_dense, model=model_sparse).to(device)\n",
    "else:\n",
    "    if experiment == \"fully trained\":\n",
    "        sparse_path = \"haraghi/sweep EST (FAN1VS3) (multi val test num 20)/bnq6i70m\"\n",
    "        dense_path = \"haraghi/sweep EST (FAN1VS3) 25000 (multi val test num 20)/no47hfm9\"\n",
    "    elif experiment == \"5 epoch trained\":\n",
    "        sparse_path = \"haraghi/DGCNN/x0x9h3ux\"\n",
    "        dense_path = \"haraghi/DGCNN/yq3e10ls\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown experiment: {experiment}\")\n",
    "\n",
    "    checkpoint_file_sparse = get_checkpoint_file(*sparse_path.split(\"/\"))\n",
    "    checkpoint_file_dense = get_checkpoint_file(*dense_path.split(\"/\"))\n",
    "    cfg_sparse,_ = get_config_file(*sparse_path.split(\"/\"))\n",
    "    cfg_dense,_ = get_config_file(*dense_path.split(\"/\"))\n",
    "    model_sparse = model_factory.factory(cfg_sparse)\n",
    "    model_dense = model_factory.factory(cfg_dense)\n",
    "    runner_dense = Runner.load_from_checkpoint(checkpoint_path=checkpoint_file_dense, cfg=cfg_sparse, model=model_dense).to(device)\n",
    "    runner_sparse = Runner.load_from_checkpoint(checkpoint_path=checkpoint_file_sparse, cfg=cfg_sparse, model=model_sparse).to(device)\n",
    "    \n",
    "    \n",
    "assert cfg_dense.transform.train.transform, \"The dense model must have transform enabled\"\n",
    "assert cfg_dense.transform.train.num_events_per_sample == 25000, \"The dense model must have num_events_per_sample=25000\"\n",
    "assert not cfg_dense.pre_transform.train.num_events_per_sample, \"The dense model must have num_events_per_sample disabled\"\n",
    "assert not cfg_dense.pre_transform.train.transform, \"The dense model must have transform disabled\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "datasets_path = 'datasets_torch_geometric'\n",
    "sparse_dataset =   create_dataset(\n",
    "                dataset_path = os.path.join(datasets_path, cfg_sparse.dataset.name, 'data'),\n",
    "                dataset_name  = cfg_sparse.dataset.name,\n",
    "                dataset_type = 'training',\n",
    "                transform = transforms(cfg_sparse.transform.train),\n",
    "                pre_transform=transforms(cfg_sparse.pre_transform.train),\n",
    "                num_workers=3\n",
    "            )\n",
    "dense_dataset =   create_dataset(\n",
    "                dataset_path = os.path.join(datasets_path, cfg_dense.dataset.name, 'data'),\n",
    "                dataset_name  = cfg_dense.dataset.name,\n",
    "                dataset_type = 'training',\n",
    "                transform = transforms(cfg_dense.transform.train),\n",
    "                pre_transform=transforms(cfg_dense.pre_transform.train),\n",
    "                num_workers=3\n",
    "            )\n",
    "\n",
    "cfg_sparse.dataset.num_classes = sparse_dataset.num_classes\n",
    "cfg_dense.dataset.num_classes = dense_dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(10):\n",
    "    a.append(dense_dataset[0].pos[:2,0])\n",
    "    \n",
    "print(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# idx_list = [np.random.randint(len(dense_dataset))]\n",
    "idx_list = [181]\n",
    "results_dict = {}\n",
    "for case in ['sparse', 'dense']:\n",
    "    runner = runner_sparse if case == 'sparse' else runner_dense\n",
    "    grads = {}\n",
    "    ws = {}\n",
    "    losses = []\n",
    "    for idx in idx_list:\n",
    "        for i in range(100):\n",
    "            runner.model.zero_grad()\n",
    "            data = eval(case + '_dataset')[idx]\n",
    "            # if case == 'dense':  \n",
    "            #     num_nodes = data.num_nodes\n",
    "            #     perm = torch.randperm(num_nodes, requires_grad=False)\n",
    "            #     data.pos = data.pos[perm,:]\n",
    "            #     data.x = data.x[perm,:] \n",
    "            #     data.pos = data.pos[1000:,:]\n",
    "            #     data.x = data.x[1000:,:]\n",
    "            \n",
    "            data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "            data = data.to(device)\n",
    "            loss, out = runner._step(data)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            if i == 0:\n",
    "                ws['conv1'] = []\n",
    "            ws['conv1'].append(runner.model.classifier.conv1.weight.data.clone().detach().cpu().view(-1,1).numpy())\n",
    "            if i > 0:\n",
    "                assert np.allclose(ws['conv1'][-1], ws['conv1'][-2]), 'conv1 changed'\n",
    "            for l in range(1,5):\n",
    "                for name, param in getattr(runner.model.classifier, f'layer{l}')[-1].named_parameters():\n",
    "                    if 'conv2' in name:\n",
    "                        if i == 0:\n",
    "                            ws[f'layer{l}_conv2'] = []\n",
    "                        ws[f'layer{l}_conv2'].append(param.data.clone().detach().cpu().view(-1,1).numpy())\n",
    "                        if i > 0:\n",
    "                            assert np.allclose(ws[f'layer{l}_conv2'][-1], ws[f'layer{l}_conv2'][-2]), f'layer{l}_conv2 changed'\n",
    "            if i == 0:\n",
    "                grads['conv1'] = []\n",
    "            grads['conv1'].append(runner.model.classifier.conv1.weight.grad.clone().detach().cpu().view(-1,1).numpy())\n",
    "            for l in range(1,5):\n",
    "                for name, param in getattr(runner.model.classifier, f'layer{l}')[-1].named_parameters():\n",
    "                    if 'conv2' in name:\n",
    "                        if i == 0:\n",
    "                            grads[f'layer{l}_conv2'] = []\n",
    "                        grads[f'layer{l}_conv2'].append(param.grad.clone().detach().cpu().view(-1,1).numpy())\n",
    "\n",
    "    for k in grads.keys():\n",
    "        grads[k] = np.concatenate(grads[k], axis=1)\n",
    "    # for k in ws.keys():\n",
    "    #     ws[k] = np.concatenate(ws[k], axis=1)\n",
    "    results_dict[case] = {}\n",
    "    results_dict[case]['grads'] = grads\n",
    "    # results_dict[case]['ws'] = ws\n",
    "    results_dict[case]['losses'] = np.array(losses)\n",
    "\n",
    "losses_dict[experiment] = {}\n",
    "vec_sum_dict[experiment] = {}\n",
    "norm_mean_dict[experiment] = {}\n",
    "cosine_sim_dict[experiment] = {}\n",
    "\n",
    "for case in ['sparse', 'dense']:\n",
    "    print(f'Case: {case}')\n",
    "    grads_vec = results_dict[case]['grads']\n",
    "    losses_dict[experiment][case] = results_dict[case]['losses']\n",
    "    vec_sum_dict[experiment][case] = []\n",
    "    cosine_sim_dict[experiment][case] = []\n",
    "    norm_mean_dict[experiment][case] = []\n",
    "    # Calculate the cosine similarity matrix\n",
    "    for k,v in grads_vec.items():\n",
    "        print(f'Layer: {k}')\n",
    "        cosine_sim_dict[experiment][case].append(cosine_similarity(v))\n",
    "        vec_sum_dict[experiment][case].append(np.linalg.norm(np.sum(v, axis=1)))\n",
    "        norm_mean_dict[experiment][case].append(np.mean(np.linalg.norm(v, axis=0)))\n",
    "for case in ['sparse', 'dense']:\n",
    "    print(f'Case: {case}')\n",
    "    grads_vec = results_dict[case]['grads']\n",
    "    all_sum = []\n",
    "    all_pow_2 = []\n",
    "    for k,v in grads_vec.items():\n",
    "        all_sum.append(np.sum(v, axis=1))\n",
    "        all_pow_2.append(np.sum(v**2, axis=0))\n",
    "    all_sum = np.concatenate(all_sum, axis=0)\n",
    "    all_pow_2 = np.sum(all_pow_2, axis=0)\n",
    "    # print(all_pow_2.shape)\n",
    "    vec_sum_dict[experiment][case].append(np.linalg.norm(all_sum))\n",
    "    norm_mean_dict[experiment][case].append(np.mean(np.sqrt(all_pow_2)))\n",
    "    cosine_sim_dict[experiment][case].append(cosine_similarity(list(grads_vec.values())))\n",
    "vec_sum_df[experiment] = pd.DataFrame(vec_sum_dict[experiment], index=[*results_dict['sparse']['grads'].keys(), 'all'])\n",
    "norm_mean_df[experiment] = pd.DataFrame(norm_mean_dict[experiment], index=[*results_dict['sparse']['grads'].keys(), 'all']) \n",
    "cosine_sim_df[experiment] = pd.DataFrame(cosine_sim_dict[experiment], index=[*results_dict['sparse']['grads'].keys(), 'all'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = os.path.join('images', 'paper', 'grad_diversity')\n",
    "with open(os.path.join(folder_dir, 'grad_diversity_results.pkl'), 'wb') as f:\n",
    "    pickle.dump((vec_sum_df, norm_mean_df, cosine_sim_df), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = os.path.join('images', 'paper', 'grad_diversity')\n",
    "with open(os.path.join(folder_dir, 'grad_diversity_results.pkl'), 'rb') as f:\n",
    "    vec_sum_df, norm_mean_df, cosine_sim_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "cases = ['sparse', 'dense']\n",
    "for exper, cos_sim in cosine_sim_df.items():\n",
    "    print(exper)\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(30, 4))\n",
    "    axis_idx = 0\n",
    "    for layer in cos_sim.index[:-1]:\n",
    "\n",
    "        upper_triangle_list = []\n",
    "        \n",
    "        for case in cases:\n",
    "            # Plot the first image \n",
    "            upper_triangle = np.triu(cos_sim[case][layer])\n",
    "            upper_triangle_list.append(upper_triangle[np.nonzero(upper_triangle)])\n",
    "            axs[axis_idx].hist(upper_triangle[np.nonzero(upper_triangle)], bins=40, alpha=0.7, label=case, range=(-0.2,1) ,edgecolor='none', density=True ) \n",
    "        # axs[axis_idx].set_xlabel(f\"Histogram of Upper Triangle Cosine Similarity\", fontsize=14)\n",
    "        # axs[axis_idx].set_ylabel('Frequency', fontsize=14)\n",
    "        \n",
    "        axs[axis_idx].set_yticks([])  # Remove x-axis ticks\n",
    "        axs[axis_idx].set_xticks([-.2,0,0.2,0.4,0.6,0.8,1.0])  # Remove x-axis ticks\n",
    "        axs[axis_idx].tick_params(axis='both', labelsize=24)  # Set tick label font size      \n",
    "        axs[axis_idx].grid(axis='x', linestyle='--')  # Add pale and dashed grid lines along the x-axis\n",
    "        axs[axis_idx].legend(fontsize=24)\n",
    "        axs[axis_idx].set_title(f\"Layer name: {layer}\", fontsize=28) # number of parameters: {results_dict[case]['grads'][layer].shape[0]}\")\n",
    "        axis_idx += 1\n",
    "\n",
    "        # Adjust the spacing between subplots\n",
    "    # fig.suptitle(f\"For {exper} models\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(osp.join(folder_dir,f\"Histogram_cosine_similarity_per_layer_{exper}.png\"))\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
