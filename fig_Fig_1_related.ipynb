{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "from transform_factory import factory as transforms\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from models.est import create_image\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from utils.config_utils import get_checkpoint_file, get_config_file, show_cfg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from models.est import create_image\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = api.projects(entity=\"haraghi\")\n",
    "for project in projects:\n",
    "    print(project.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name_and_num_classes = {\n",
    "    \"NCARS\": {\"name\": \"N-Cars\", \"num_classes\": 2},\n",
    "    \"NASL\": {\"name\": \"N-ASL\", \"num_classes\": 24},\n",
    "    \"NCALTECH101\": {\"name\": \"N-Caltech101\", \"num_classes\": 101},\n",
    "    \"DVSGESTURE_TONIC\": {\"name\": \"DVS-Gesture\", \"num_classes\": 11},\n",
    "    \"FAN1VS3\": {\"name\": \"Fan1vs3\", \"num_classes\": 2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_projects = [\n",
    "        \"FINAL-NASL-varyinig-sparsity\",\n",
    "        \"FINAL-NCARS-varyinig-sparsity\",\n",
    "        \"FINAL-DVSGESTURE_TONIC-HP-varyinig-sparsity\",\n",
    "        \"FINAL-FAN1vs3-varyinig-sparsity\",\n",
    "        \"FINAL-NCALTECH101-varyinig-sparsity\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_val_and_test_acc_keys(run):\n",
    "    val_acc_key = []\n",
    "    test_acc_key = []\n",
    "    for key in run.summary.keys():\n",
    "        if \"val\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            val_acc_key.append(key)\n",
    "        if \"test\" in key and \"acc\" in key and \"mean\" in key:\n",
    "            test_acc_key.append(key)\n",
    "    assert len(val_acc_key) <= 1, f\"More than one val acc key found: {val_acc_key}\"\n",
    "    assert len(test_acc_key) <= 1, f\"More than one test acc key found: {test_acc_key}\"\n",
    "    return val_acc_key[0] if len(val_acc_key) == 1 else None , test_acc_key[0] if len(test_acc_key) == 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'paper'\n",
    "subfolder_name = os.path.join('images',folder_name,'fig_1')\n",
    "entity = 'haraghi'\n",
    "if not os.path.exists(subfolder_name):\n",
    "    os.makedirs(subfolder_name)\n",
    "\n",
    "test_dict = {}\n",
    "num_events_set = set() \n",
    "\n",
    "for project_name in dataset_projects:\n",
    "    runs = api.runs(f\"{entity}/{project_name}\")\n",
    "    runs = [r for r in runs if r.state == \"finished\" and \"transform\" in r.config]\n",
    "    if len(runs) == 0:\n",
    "        print(f\"No runs found for {project_name}\")\n",
    "        continue\n",
    "    num_events = np.unique([run.config['transform']['train']['num_events_per_sample'] for run in runs])\n",
    "    runs_per_num_events = {num_event: [run for run in runs if run.config['transform']['train']['num_events_per_sample'] == num_event] for num_event in num_events}\n",
    "    dataset_name = runs[0].config[\"dataset\"][\"name\"]\n",
    "    \n",
    "    num_events_set = num_events_set.union(set(num_events))\n",
    "    \n",
    "    test_mean = {}\n",
    "    test_max = {}\n",
    "    for num_event in num_events:\n",
    "        test_mean[num_event] = []\n",
    "        tes_max_val = -1\n",
    "        for run in runs_per_num_events[num_event]:\n",
    "            _, test_key = find_val_and_test_acc_keys(run)\n",
    "            if test_key in run.summary: \n",
    "                test_mean[num_event].append(run.summary[test_key])\n",
    "                if run.summary[test_key] > tes_max_val:\n",
    "                    tes_max_val = run.summary[test_key]\n",
    "                    test_max[num_event] = (run.summary[test_key], run)\n",
    "            else:\n",
    "                test_mean[num_event].append(None)\n",
    "            \n",
    "        print(f\"percentage of runs with test acc for {num_event} events: {np.sum([v is not None for v in test_mean[num_event]]) / len(test_mean[num_event])} out of {len(test_mean[num_event])} runs\")\n",
    "    test_mean_val = {}\n",
    "    for num_event in num_events:\n",
    "        test_mean_val[num_event] = (np.mean(test_mean[num_event]),test_max[num_event][1])\n",
    "    test_dict[dataset_name] = [test_mean_val  ,test_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for dataset_name, test_results in test_dict.items(): \n",
    "    print(f\"Dataset: {dataset_name}\", flush=True)\n",
    "    if dataset_name == \"NASL\":\n",
    "        datasets[dataset_name] =   create_dataset(\n",
    "            dataset_path = os.path.join(\"datasets_torch_geometric\", dataset_name, 'data'),\n",
    "            dataset_name  = dataset_name,\n",
    "            dataset_type = 'test',\n",
    "            transform = None,#transforms(cfg.transform.train),\n",
    "            pre_transform = None,# transforms(cfg.pre_transform.train),\n",
    "            num_workers=3\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_ids = [\n",
    "        # \"c_0371.mat\",\n",
    "        \"v_0544.mat\",\n",
    "        \"w_1835.mat\",\n",
    "        # \"w_0034.mat\"\n",
    "]\n",
    "sample_ids = []\n",
    "for i,d in enumerate(datasets[\"NASL\"]):\n",
    "    if d.file_id in sample_file_ids:\n",
    "        print(f\"Sample id: {d.file_id} found at index {i}\")\n",
    "        sample_ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_EST = False\n",
    "class_label = 'w'\n",
    "device = torch.device(\"cpu\")\n",
    "num_event_list = [16, 32, 64, 1024, 25000]\n",
    "size_list = [40, 40, 20, 10, 5]\n",
    "alpha_list = [1.0, 1.0, 1.0, 0.9, 0.7]\n",
    "size_dict = dict(zip(num_event_list, size_list))\n",
    "alpha_dict = dict(zip(num_event_list, alpha_list))\n",
    "# Create custom colormaps\n",
    "cmap_blue = mcolors.LinearSegmentedColormap.from_list(\"blue_cmap\", [\"white\", \"blue\"])\n",
    "cmap_red = mcolors.LinearSegmentedColormap.from_list(\"red_cmap\", [\"white\", \"red\"])\n",
    "file_path_dict = {}\n",
    "for dataset_name, test_results in test_dict.items(): \n",
    "    dataset =  datasets[dataset_name]\n",
    "    # for _ in range(10):\n",
    "    file_path_dict[dataset_name] = {}\n",
    "    for sample_id in sample_ids:\n",
    "        file_path_dict[dataset_name][dataset[sample_id].label[0]] = {}\n",
    "        # while True:\n",
    "        #     sample_id = np.random.randint(len(dataset))\n",
    "        #     if dataset[sample_id].label[0].lower() == class_label.lower():\n",
    "        #         break\n",
    "        print(f\"sample_id: {sample_id} class: {dataset[sample_id].label[0]} file_id: {dataset[sample_id].file_id}\")\n",
    "        for num_event, test_max_run in test_results[0].items():\n",
    "            if num_event not in num_event_list:\n",
    "                continue\n",
    "            print(num_event)\n",
    "            run = test_max_run[1]     \n",
    "            cfg,_ = get_config_file(run.entity, run.project, run.id, verbose=False)\n",
    "            \n",
    "            H, W = cfg.dataset.image_resolution\n",
    "            cfg.transform.test.filter_nodes = \"remove_NASL_failed_pixels\"\n",
    "            dataset.transform = transforms(cfg.transform.test) \n",
    "            data = dataset[sample_id]\n",
    "            vox = torch.zeros(2*H*W)\n",
    "            # get values for each channel\n",
    "            x, y, p = data.pos[:,0], data.pos[:,1], data.x[:,0]\n",
    "            p = (p+1)/2  # maps polarity to 0, 1\n",
    "            idx =         x.int() \\\n",
    "                        + W * y.int()\\\n",
    "                        + W * H * p.int()\n",
    "            vox.put_(idx.long(), vox.new_full([data.num_nodes,], fill_value=1), accumulate=True)\n",
    "            vox = vox.view(2, H, W)\n",
    "            vox_sum = vox.sum(0)\n",
    "            fig, ax = plt.subplots()\n",
    "            \n",
    "            # Choose a colormap\n",
    "            # cmap = plt.cm.viridis\n",
    "            # nonzero_indices = torch.nonzero(vox_sum, as_tuple=True)\n",
    "            # x_coords = nonzero_indices[0].numpy()\n",
    "            # y_coords = nonzero_indices[1].numpy()\n",
    "            # values = vox_sum[x_coords, y_coords]\n",
    "            # norm = (values - values.min()) / (values.max() - values.min())\n",
    "            # colors = cmap(norm.numpy())\n",
    "            # scatter_ax = ax[0].scatter(y_coords, x_coords, c=colors, s=np.ones_like(x_coords)*1, alpha=1.0)\n",
    "            \n",
    "            nonzero_indices = torch.nonzero(vox[0], as_tuple=True)\n",
    "            x_coords = nonzero_indices[0].numpy()\n",
    "            y_coords = nonzero_indices[1].numpy()\n",
    "            values = vox[0,x_coords, y_coords]\n",
    "            norm = values / values.max()\n",
    "            colors = cmap_blue(norm.numpy())\n",
    "            scatter_ax = ax.scatter(y_coords, x_coords, c=colors, s=np.ones_like(x_coords)*size_dict[num_event], alpha=alpha_dict[num_event])\n",
    "            cbar_blue = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=values.min(), vmax=values.max()), cmap=cmap_blue)\n",
    "            \n",
    "            \n",
    "            nonzero_indices = torch.nonzero(vox[1], as_tuple=True)\n",
    "            x_coords = nonzero_indices[0].numpy()\n",
    "            y_coords = nonzero_indices[1].numpy()\n",
    "            values = vox[1,x_coords, y_coords]\n",
    "            norm = values / values.max()\n",
    "            colors = cmap_red(norm.numpy())\n",
    "            scatter_ax_2 = ax.scatter(y_coords, x_coords, c=colors, s=np.ones_like(x_coords)*size_dict[num_event], alpha=alpha_dict[num_event])\n",
    "            cbar = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=values.min(), vmax=values.max()), cmap=cmap_red)\n",
    "            # cbar.set_label('Value')\n",
    "\n",
    "            ax.set_aspect('equal','box')\n",
    "            ax.set_xlim(-0.5, vox_sum.shape[1] - 0.5)\n",
    "            ax.set_ylim(-0.5, vox_sum.shape[0] - 0.5)\n",
    "            # ax.set_title(\"Acc. = {:.2f} %\".format(test_max_run[0]*100), fontweight='bold', fontsize=24)\n",
    "            \n",
    "            \n",
    "            # accumulate_ax = ax[1].imshow(vox_sum)\n",
    "            # ax[1].set_aspect('equal','box')\n",
    "            # ax[1].invert_yaxis()\n",
    "\n",
    "            # ax_names = [cbar, accumulate_ax]\n",
    "            # if after_EST:\n",
    "            #     model = model_factory.factory(cfg).to(device)\n",
    "            #     checkpoint_file = get_checkpoint_file(run.entity, run.project, run.id)\n",
    "            #     runner = Runner.load_from_checkpoint(checkpoint_path=checkpoint_file, cfg=cfg, model=model, map_location=device)\n",
    "            #     data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "            #     data = data.to(device)\n",
    "            #     runner.model.eval()\n",
    "            #     with torch.no_grad():\n",
    "            #         vox_after_est = runner.model.quantization_layer.forward(data)\n",
    "            #     vox_after_est = create_image(vox_after_est)\n",
    "            #     est_ax = ax[2].imshow(vox_after_est.numpy().transpose(1,2,0), cmap='viridis')\n",
    "            #     ax_names.append(est_ax)\n",
    "            #     ax[2].invert_yaxis()\n",
    "            # Show plot\n",
    "            \n",
    "            # for i, ax_name in enumerate(ax_names):\n",
    "            ax.axis('off')\n",
    "                    # Remove labels and ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('')\n",
    "            # ax.set_title('')\n",
    "            ax.spines['top'].set_linewidth(2.0)\n",
    "            ax.spines['right'].set_linewidth(2.0)\n",
    "            ax.spines['bottom'].set_linewidth(2.0)\n",
    "            ax.spines['left'].set_linewidth(2.0)\n",
    "            plt.tight_layout(pad=1.0)\n",
    "            # fig.colorbar(ax_name, ax=ax[i])\n",
    "            file_name = f\"{dataset_name}_{num_event}_{dataset[sample_id].label[0]}.png\"\n",
    "            file_path_dict[dataset_name][dataset[sample_id].label[0]][num_event] = [file_name, test_max_run[0]]\n",
    "            plt.savefig(os.path.join(subfolder_name, file_name), bbox_inches='tight')\n",
    "            print(f\"Saving {file_name}\")\n",
    "            plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fig1(file_path, file_path_dict, num_event_list, test_mean_list):\n",
    "    # Open file for writing\n",
    "    with open(file_path, \"w\") as file:\n",
    "        # Write table header\n",
    "        file.write(\"\\\\begin{tabular}{@{}l\"+(\"@{}M{30mm}\"*(len(num_event_list)))+\"@{}}\\n\")\n",
    "        file.write(\"\\\\toprule\\n\")\n",
    "        file.write(\"\\\\textbf{\\\\small Test Acc.} & \" +\n",
    "                   \" & \".join([\"\\\\textbf{{{:.2f}\\%}}\".format(t_m * 100) for t_m in test_mean_list]) +\n",
    "                   \"\\\\\\\\\\n\")\n",
    "        file.write(\"\\\\textbf{\\# events per video} & \" +\n",
    "                    \" & \".join([f\"\\\\textbf{{{n_e}}}\" for n_e in num_event_list]) +\n",
    "                    \"\\\\\\\\\\n\")\n",
    "        file.write(\"\\\\midrule\\n\")\n",
    "\n",
    "        # Write table rows\n",
    "        for class_label, values in file_path_dict.items():\n",
    "                \n",
    "            row = \"{Class `\" + class_label.upper() + \"\\'} & \"#\"\\\\rotatebox{90}\n",
    "            # Number of classes\n",
    "            row += \" & \".join([\"\\\\includegraphics[width=25mm]{images/fig_1/\" + v[0] + \"}\" for v in values.values()]) + \"\\\\\\\\\\n\"\n",
    "            file.write(row)\n",
    "        \n",
    "        file.write(\"\\\\bottomrule\\n\")\n",
    "        file.write(\"\\\\end{tabular}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(subfolder_name, \"fig_1.tex\")\n",
    "if 'c' in file_path_dict['NASL']:\n",
    "    file_path_dict['NASL'].pop('c')\n",
    "ll = [l for l in file_path_dict['NASL'].keys()]\n",
    "num_event_list = [nn for nn in file_path_dict['NASL'][ll[0]]]\n",
    "test_mean_list = [file_path_dict['NASL'][ll[0]][num_event][1] for num_event in num_event_list]\n",
    "write_fig1(file_path, file_path_dict['NASL'], num_event_list, test_mean_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
