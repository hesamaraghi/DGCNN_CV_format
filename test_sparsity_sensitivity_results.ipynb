{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import model_factory\n",
    "from graph_data_module import GraphDataModule\n",
    "from train import Runner\n",
    "from datasets_torch_geometric.dataset_factory import create_dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"haraghi\"\n",
    "project = \"DGCNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls = torch.load(\"sparsity_sensitivity_results_nasl5.pt\")\n",
    "tested_num_events = []\n",
    "trained_num_events = []\n",
    "for dataset_name, dataset_results in resutls.items():\n",
    "    print(dataset_name)\n",
    "    for run_id, run_results in dataset_results.items():\n",
    "        if run_results['trained_num_events'] is not None:\n",
    "            trained_num_events.append(f\"{run_id},{run_results['trained_num_events']},({run_results['summary_test_acc']:.4f})\")\n",
    "        tested_num_events.extend(list(run_results[\"tested_num_events\"].keys()))\n",
    "\n",
    "\n",
    "    tested_num_events = sorted(list(set(tested_num_events)))\n",
    "    trained_num_events = sorted(trained_num_events, key=lambda x: int(x.split(\",\")[1]))\n",
    "\n",
    "    mat = []\n",
    "    mat_dict = {}\n",
    "    for row in trained_num_events:\n",
    "        row_results = []\n",
    "        run_id = row.split(\",\")[0]\n",
    "        mat_dict[run_id] = {}\n",
    "        for col in tested_num_events:\n",
    "            if col in resutls[dataset_name][run_id]['tested_num_events']:\n",
    "                row_results.append(     [t_r[0]['test/acc'] for t_r in dataset_results[run_id]['tested_num_events'][col]])\n",
    "                mat_dict[run_id][col] = [t_r[0]['test/acc'] for t_r in dataset_results[run_id]['tested_num_events'][col]]\n",
    "                test_number = len(dataset_results[run_id]['tested_num_events'][col])\n",
    "            else:\n",
    "                row_results.append(None)\n",
    "                mat_dict[run_id][col] = None\n",
    "        mat.append(row_results)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in enumerate(mat):\n",
    "    for j,c in enumerate(r):\n",
    "        if c is None:\n",
    "            mat[i][j] = [np.nan]*test_number\n",
    "\n",
    "mat = np.array(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_num_events_to_exclude = [10,12,14]\n",
    "trained_models_to_exclude = ['icey2rjl','7zx0vpka']\n",
    "\n",
    "\n",
    "# Define the rows and columns to exclude\n",
    "rows_to_exclude = [i for i,t in enumerate(trained_num_events) if t.split(\",\")[0] in trained_models_to_exclude]\n",
    "cols_to_exclude = [i for i,t in enumerate(tested_num_events) if t in tested_num_events_to_exclude]\n",
    "\n",
    "trained_num_events = [t for i,t in enumerate(trained_num_events) if i not in rows_to_exclude]\n",
    "tested_num_events = [t for i,t in enumerate(tested_num_events) if i not in cols_to_exclude]\n",
    "\n",
    "\n",
    "print(mat.shape)\n",
    "# Exclude the rows and columns from mat\n",
    "mat_filtered = np.delete(mat, rows_to_exclude, axis=0)\n",
    "print(mat_filtered.shape)\n",
    "mat_filtered = np.delete(mat_filtered, cols_to_exclude, axis=1)\n",
    "print(mat_filtered.shape)\n",
    "mat_mean = np.nanmean(mat_filtered, axis=2)\n",
    "mat_std = np.nanstd(mat_filtered, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "figure = plt.figure(figsize=(6, 12))\n",
    "ax = plt.gca()\n",
    "im = ax.imshow(mat_mean)\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(tested_num_events)), labels=tested_num_events)\n",
    "ax.set_yticks(np.arange(len(trained_num_events)), labels=trained_num_events)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(trained_num_events)):\n",
    "    for j in range(len(tested_num_events)):\n",
    "        text = ax.text(j, i, f'{100*mat_mean[i, j]:.2f}\\nÂ±{100*mat_std[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.colorbar(im, fraction=0.04, pad=0.04)\n",
    "plt.xlabel(\"Tested number of events\")\n",
    "plt.ylabel(\"Trained number of events\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
